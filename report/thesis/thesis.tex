% Do not change document class, margins, fonts, etc.
\documentclass[a4paper,oneside,bibliography=totoc]{scrbook}
\setlength{\parindent}{25pt}
\newtheorem{definition}{Definition} \newtheorem{proposition}{Proposition}
\usepackage{appendix}

% Add packages
\usepackage[
colorlinks=true, urlcolor=blue, linkcolor=black, colorlinks,citecolor=green
]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{emptypage}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[justification=raggedright]{caption}
\captionsetup{labelformat=empty, textformat=empty}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{algorithm} % you can modify the algorithm style to your liking
\usepackage{algorithmic}
\usepackage{csquotes}
\renewcommand{\algorithmiccomment}[1]{\hfill\textit{// #1}}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[nopostdot, style=super, nogroupskip, nonumberlist, toc]{glossaries}
\makeglossaries

% Citation style
\usepackage{cite
\usepackage{csquotes}
\bibliographystyle{chicagoa}
\setcitestyle{authoryear,round,semicolon,aysep={},yysep={,}} \let\cite\citep

%---------------------------------------------------------------------------------------
\input{acronyms}

% Begin Documents
\begin{document}
\setlength{\skip\footins}{20pt}
% Cover page
\frontmatter \subject{Master Thesis} % change to appropriate type
\title{\LARGE 
	Uncertainty Calibration with Online Conformal Prediction in Neural Architecture Search: \\ 
	An Evaluation under the BANANAS Framework 
}
\author{
	Cheng Chen\\ (matriculation number 1662473)} \date{July 31, 2025
}
\publishers{
	{\small Submitted to}\\
	Data and Web Science Group\\Prof.\ Dr.\ Margret Keuper\\University of Mannheim\\
}
\maketitle

% Abstract
\chapter{Abstract}



% Table of contents
\begingroup%
\hypersetup{hidelinks} % disable link color in TOC only
\tableofcontents%
\endgroup


% Body
\mainmatter  % start new numbering

% chapter 1
\chapter{Introduction}
\label{ch:intro}

\section{Motivation}
Over the last decades, deep learning has achieved remarkable success in a variety of areas, including computer vision, natural language processing, and machine translation. This success is partly attributed to the carefully designed neural network architectures. Driven by the rising demand for efficient architecture engineering in complex domains, \gls{nas} has emerged as a technique for automating the design of task-specific neural architectures.

Recently, a \gls{nas} framework “Bayesian optimization + neural predictor” has been developed and demonstrated strong efficiency. In particular, \cite{white2019bananas}  performs a thorough evaluation on the framework and develops the search algorithm BANANAS based on the analyses. This \gls{nas} algorithm offers both effectiveness and scalability and has achieved state-of-the-art performance on popular benchmarks. However, BANANAS still replies on a Gaussian assumption for uncertainty estimation, which is a critical step in the Bayesian optimization process. This assumption does not necessarily hold and thus may restrict the effectiveness of BANANAS in real-world applications

Previous studies have shown that accurate uncertainty estimates improve the performances of deep learning models \cite{pmlr-v80-kuleshov18a}. In particular, \cite{deshpande2024online} also applies uncertainty calibration in the context of Bayesian optimization and demonstrates that calibrated Bayesian optimization converges to better optima in fewer steps. Motivated by prior findings, we seek to enhance the uncertainty quantification method used in BANANAS. To mitigate the limitation of relying on the Gaussian assumption, this work proposes to incorporate \gls{cp},  a model-agnostic approach for uncertainty quantification \cite{shafer2008tutorial, vovk2005algorithmic}, into the BANANAS framework, since \gls{cp} offers strong theoretical guarantee, flexibility, and practical applicability. A closely related work is \cite{salinas2023optimizing}, which introduces a framework based on conformalized quantile regression for hyperparameter tuning using Bayesian optimization. Specifically, this work employees a quantile regressor as the surrogate and calibrates quantile estimates using \gls{cp} so that the model can handle non-Gaussian or heteroskedastic observation noises. Empirical evaluations show that this innovative operation can yield more robust performances compared to other state-of-the-art methods.

\section{Contributions and Limitations}
In this work, we review various \gls{cp} methods and identify several algorithms that are suitable for BANANAS and can be integrated into the existing framework with little overhead. Based on the evaluations, we propose a new framework BANANAS-CP, which produces \gls{cp} calibrated distribution estimates and tracks calibration performance at each epoch in the architecture search procedure. 

Specifically, we carefully select three different \gls{cp} methods for uncertainty calibration: \gls{scp}, \gls{cvcp}, \gls{btcp}. As for the surrogate model in Bayesian optimization, we consider two different prediction algorithms: an ensemble predictor and a quantile regressor. These configurations result in a total of five distinct methods for neural architecture search. Although only two types of surrogate models are explored in this work, this framework in fact can be combined with any prediction algorithm that produces quantile estimates for any specified quantile levels, either based on a specific distribution assumption or via a probabilistic modeling approach. In addition, BANANAS-CP is highly flexible and can be applied with any acquisition function since it models the shape of the full conditional distribution for each data point. In contrast, the work based on conformalized quantile regression \cite{salinas2023optimizing}, which is mentioned in the previous section, only produces discrete quantile estimates and thereby can not be applied with acquisition functions relying on a continuous distribution, like Expected Improvement.

We evaluate the performance of BANANAS-CP on the commonly used benchmark dataset NAS-Bench-201. Empirical results show that, in general, incorporating an uncertainty calibration step into the existing framework can significantly reduce calibration errors, suggesting enhanced uncertainty estimates. However, better uncertainty estimates can not always be efficiently translated into better search performance. Among the five variants of calibrated search methods, only \gls{cvcp} with a quantile regressor and a specific acquisition function has achieved better or comparable performances than the original BANANAS method. Despite of the  less encouraging empirical outcomes, our analyses provide insights intp the effect of uncertainty estimation on search performance within the BANANAS framework:


\textcolor{red}{limitations}
- small data set


\section{Outline}
Having gained an overview of the research question and the background, the remainder of this thesis is organized as follows. First, Chapter 2 reviews the related works on neural architecture search, uncertainty quantification, and in particular, conformal prediction. In Chapter 3, after proposing a novel framework to incorporate uncertainty calibration into the architecture search process in Section \ref{sec:overview}, we describe its methodological steps in more detail. In Section \ref{sec:cp}, we identify different types of conformal prediction algorithms that are applicable for \gls{nas}, and consider the use of the underlying surrogate models. In Section \ref{sec:distest} and Section \ref{sec:acq}, we further examine how the calibrated predictions can be incorporated into a Bayesian optimization process. In Chapter 4, we present an overview of the general experiment setups and the strategy for progressively tuning configurations, along with a description of the benchmark dataset used for evaluation. In Chapter 5, we present the experimental results and compare the performance of the algorithms with the pure BANANAS method. Finally, Chapter 6 concludes this work and discusses potential future directions.  

% chapter 2
\include{background}

% chapter 3
\include{methodology}

% chapter 4
\chapter{Experiment Design}
\label{ch4}
To compare the performance of BANANAS--CP with the original BANANAS method and assess the role of uncertainty calibration, we choose the widely used tabular benchmark dataset NAS-Bench-201 \cite{dong2020nasbench201} for experiments. In this chapter, we first provide a description of the dataset (Section \ref{sec: dataset}). Then, we introduce the general setups that are shared across all experiments, along with the strategy for step-wise configuration tuning  (Section \ref{sec: setups}).

\section{Dataset}
\label{sec: dataset}
NAS-Bench-201 is a cell-based architecture search space. Each cell is expressed as a densely connected \gls{dag} with in total 4 nodes and 6 edges. The nodes within a cell represents the sum of all feature maps transformed through the associated operations of the edges pointing to this node, and the edges represent the architectures operation that are chosen from the predefined operation set. Specifically, the operation set comprises 5 representative operations: (1) zeroize, (2) skip connection, (3) 1-by-1 convolution, (4) 3-by-3 convolution, and (5) 3-by-3 average pooling layer. This search space contains all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 cell candidates in total. The macro structure of an architecture is defined as a chain of blocks, which is initiated with one 3-by-3 convolution with 16 output channels and a batch normalization layer, and consists of three stacks of cells that are connected by a residual layer. Figure \ref{fig: nasbench201} illustartes the structure of an architecture in this search space.  
	\vspace{0.5em}	
	\begin{figure}[bthp]
		\centering
		\includegraphics[scale=0.48]{figs/nas_bench_201.png}
		\refstepcounter{figure}
   		\addcontentsline{lof}{figure}{Figure~\thefigure: Illustration of the Overall Network Architecture Structure in NAS-Bench-201}
		\label{fig: nasbench201}
			\parbox{\linewidth}{
	 		\vspace{0.7em}
 	 		{\small \textit{Figure \ref{fig: nasbench201}:} Illustration of the skeleton (top) and the design of individual cells (bottom) of architectures in NAS-Bench-201 \cite{dong2020nasbench201}.
 	 		}
 		}
	\end{figure}
\newline

\newline
Architectures in the search space are evaluated on three datasets that are widely used for image classification tasks: CIFAR10, CIFAR100 \cite{krizhevsky2009learning} and ImageNet16-120 \cite{chrabaszcz2017downsampled}. Each dataset is split into the training, validation, and test sets using a standard evaluation pipeline. NAS-Bench-201 provides the training, validation, and test losses as well as accuracies for all architectures in the search space. The following gives a brief introduction to these datasets:

\begin{description}[leftmargin=0cm, listparindent=\parindent]
 	\item[CIFAR10]:	The dataset consists of 60K $32\times32$ color images in 10 classes. In NAS-Bench-201, 25K images with 10 classes are assigned into the training and the validation sets, respectively. The test set contains 10K images, with 1K images per class.
 	\item[CIFAR100]: This dataset has the same images as CIFAR-10 but in 100 classes. The training set has 50K images, and each of the validation and the test sets has 5K images.
 	\item[ImageNet16-120]: This dataset contains 151.7K training images, 3K validation images, and 3K test images with 120 classes. Each image has 16$\times$16 pixels.
\end{description} 


Since its introduction, NAS-Bench-201 has contributed to the \gls{nas} community in several aspects. First, it provides full training and test results (e.g., accuracy, training time, etc.) for every possible architecture in the space on three datasets, allowing reproducible NAS experiments without training models from scratch. Because all architectures are pre-evaluated, \gls{nas} methods can be benchmarked extremely efficiently and subsequent \gls{nas} research can just focus on the search algorithms without any model evaluation. Moreover, using a unified dataset splitting strategy, NAS-Bench-201 reduces the variability caused by different implementation details or training setups, which is a limitation of previous benchmarks, and thus enables consistent comparisons across different \gls{nas} algorithms. 

\vspace{0.2em}	
NAS-Bench-201 also serves as a foundation for extending benchmark datasets. In particualr, \cite{jung2023neural} evaluates all 6,466 non-isomorphic architectures in the space for robustness against adversarial attacks and common image corruptions, and introduces a dataset that includes both clean and robust accuracy values. This dataset covers adversarial attacks and corruptions of different severity levels, enabling a systematical study on how architectural variations impact robustness.

\vspace{0.2em}
In this work, we leverage the API offered by NAS-Bench-201 and directly query the pre-computed train and validation metrics of architectures \footnote{Performance metrics for training and testing on the three datasets are downloaded from NASLib: \ulr{https://github.com/automl/NASLib/tree/Develop}}. Distributions of architecture validation accuracies on each of the three datasets are illustrated in Figure \ref{fig: val_acc_201}. Notably, NAS-Bench-201 also provides several analytical metrics, such as model rankings and accuracy correlations across the three datasets, which further guide our post-hoc analysis of the experimental results.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.38]{figs/nas_bench_201_val_acc.pdf}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Distribution of Validation Accuracies of Architectures in NAS-Bench-201}
	\label{fig: val_acc_201}
	\\
 	{\small \textit{Figure \ref{fig: val_acc_201}:} Distribution of validation accuracies of architectures in NAS-Bench-201}}
\end{figure}


\section{Setups and Implementation}
\label{sec: setups}
We now present the common experimental setups used throughout this study. As introduced in Chapter \ref{ch3}, we primarily run experiments for five calibrated NAS algorithms varying in calibration techniques and/or surrogate predictors: \gls{scp} with ensemble predictor, \gls{scp} with quantile regressor, \gls{cvcp} with ensemble predictor, \gls{cvcp} with quantile regressor, and \gls{btcp} with ensemble predictor. In line with \cite{white2019bananas}, each algorithm is given a search budget of 150 epochs to ensure consistent benchmarking with BANANAS. As in previous works, the validation accuracy is used as the supervision signal to guide the search. Each algorithm is repeated for 50 trials with different random seeds and the final results are obtained by aggregating the performance across all trials. 

\vspace{0.2em}
Although training neural networks is avoided thanks to the benchmark datset, each search algorithm still involves a large number of hyperparameters, making it unrealistic to tune them all. Also, not all hyperparameters have the same impact on search performance; some might be more important than the others. Therefore, we select a subset of hyperparameters that we believe, either base on experience or preliminary testing results, are less important or already well-set, and fix their values throughout the experiments. For instance, we believe path-encoding is stronger than other architecture encoding techniques, thus architectures are always encoded using paths present in the cell in all experiments. As for the acquisition optimization strategy, the number of candidates that the acquisition function evaluates in each iteration is fixed at 100 and the maximal mutation allowed for each model is  1 in case the mutation strategy is adopted. In addition, we follow \cite{white2019bananas} and output 10 architectures to mimic the parallelized evaluation procedure.
\vspace{0.2em}

The hyperparameters are tuned progressively. In the first stage, we focus on configurations common to all algorithms. We start by conducting a thorough analysis on the baseline method, i.e., \gls{scp} with ensemble predictor, to find the optimal general setting, like the number of quantiles, the size of the initial dataset (the number of model evaluations before fitting the surrogate), the acquisition functions and the sampling strategies, etc. This optimal setting will be applied to other more advanced approaches in the next stage. Then, we turn to hyperparameter specific to each search algorithm and 
	conduct separate experiments that are  discussed in the respective sections.

\vspace{0.2em}
We borrow the implementation of BANANAS from NASLib \footnote{https://github.com/automl/NASLib}, which is a modern Python-based framework for \gls{nas} developed by the AutoML Freiburg group. NASLib is well modularized, enabling a relatively easy integration of the new calibration block. In NASLib, each \gls{nas} algorithm typically comprises a \textit{trainer} for initiating search and evaluation iterations and an \textit{optimizer} for encapsulating specific search logics, including an inherent \textit{predictor} if applicable. Specifically, \textit{trainer} serves as a generic engine and is shared across all NAS algorithms. All predictors should conform to a particular interface so that they can be invoked inside the \textit{trainer}. 

\vspace{0.2em}
Building on this structure, we add new modules for constructing the BANANAS-CP framework, including a quantile regressor, distribution estimators (along with the compatible acquisition functions), and the calibration algorithms. We leverage the existing implementation for \textit{trainer} with mild modifications on the export functionalities, aiming for a better access to intermediate outputs, such as the estimated distribution at each iteration. In addition, we also provide tools for analyzing and interpreting the experimental results.

% chapter 5
\chapter{Results}
This chapter presents the performance of the methods introduced in Chapter \ref{ch3}, evaluated using the experimental setups described in Chapter \ref{ch4}. 

\section{Baseline: SCP with Ensemble Predictor}
\label{sec: baseline}

\subsection{Search and Calibration Setting}
\label{sec: sc_setting}
To establish baseline performance, we run experiments for both the original BANANAS method and the approach incorporating \gls{scp} with an ensemble predictor on all three datasets. In this sub-section, we explore general search and calibration settings to gain a first insight of the performances of BANANAS-CP.

The pure BANANAS serves as the benchmark. We follow the best settings reported in \cite{white2019bananas}, using an ensemble of 5 \gls{fnns} with 10 initial model evaluations, along with \gls{its} and a mutation strategy that mutates the top 2 performing architectures in each epoch for computing acquisition scores. For the variant that incorporates SCP calibration, we evaluate the search performance under different configurations: number of the initial model evaluations (either 10 or 30), split rate representing the fraction of data points assigned into the calibration set (either 0.3 or 0.5), and number of quantiles (10). For example, specifying 20 quantile levels yields a percentile vector with 20 values [0, 0.05, 0.1, ..., 0.95, 1]. After applying a filtering step to remove extreme percentiles, the remaining values are used as mis-coverage rates when performing conformal prediction, and the resulting calibrated quantiles form the basis for fitting a linearly-interpolated distribution. Unless otherwise specified, 10 quantiles are used throughout the experiments, since results on both the synthetic dataset (Table \ref{table:distest}) and NAS-Bench-201 (Appendix \ref{appendix: s1}) indicate that 10 quantiles produce more robust estimates than 20 when the dataset is relatively small with up to 150 data points.

Figure \ref{fig: init_size_cal_rate_03} and Figure \ref{fig: init_size_cal_rate_05} present the performances for search strategies with different initial dataset sizes when the train-calibration splitting ratio is set at 0.3 and 0.5, respectively. Specifically, calibration error \gls{rmsce} (top), computed with Equation \ref{rmsce} when the calibration set contains at least as many quantile levels as observations, and search performance (bottom), measured by the validation accuracy of the best architecture found so far, are reported at each epoch. The line denotes the average over 50 trials, and the surrounding band represents the variation, corresponding to one standard deviation. In addition, we also report the values of mean and standard deviation of the final validation accuracy.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35]{figs/init_size_cal_rate_03.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Impact of Initial Set Size on SCP Performance (Calibration Rate=0.3)}
   	\label{fig: init_size_cal_rate_03}
	\\
	\parbox{\linewidth}{
	 \vspace{1em}
 	  	{\small \textit{Figure \ref{fig: init_size_cal_rate_03}:} Comparison of NAS performances with different initial
 	  	evaluations when calibration set contains 30\% of all data points.}}
 	}
\end{figure}

\vspace{0.3em}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35 ]{figs/init_size_cal_rate_05.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Impact of Initial Set Size on SCP Performance (Calibration Rate=0.5)}
	\label{fig: init_size_cal_rate_05}
	\\
 	\parbox{\linewidth}{
	\vspace{1em}
 	  	{\small \textit{Figure \ref{fig: init_size_cal_rate_05}:} Comparison of NAS performances with different initial
 	  	evaluations when calibration set contains 50\% of all data points.}}
 	}
\end{figure}

In terms of the final validation accuracy, pure BANANAS without calibration consistently demonstrates the strongest search performance across different configurations and different datasets. In particular, the marginal performance gains is greater when evaluated on datasets with larger variations such as ImageNet16-120. Regarding  SCP, the effect of the initial evaluation size varies across subroutines, but the standard deviation of the final validation accuracies indicates that a smaller initialization set often results in slightly more stable performance. The search trajectory suggests that this is likely because SCP with more initial evaluations has higher chance getting stuck in local maxima. 

According to \gls{rmsce}, SCP can significantly reduce the calibration errors compared to the uncalibrated baseline especially in early epochs, indicating the calibrated distribution aligns better with the empirical observations. Typically, for all methods, the variation in \gls{rmsce} across experiment repetitions decreases over the search time, and calibration consistently leads to less variant results. However, the advantage in distribution estimate of SCP does not translate into an improved search performance. Both methods, whether calibrated or not, achieve comparable validation accuracies even at early epochs.  As search progresses, \gls{rmsce} of all methods converge closer.  In certain cases, the uncalibrated method even achieves lower calibration error toward the end of the search.

Although it does not outperforms the pure, uncalibrated BANANAS method, \gls{scp} with 10 initial evaluations and 50\% data for calibration in general demonstrates the strongest performance among all \gls{scp} approaches, achieving the best and most stable validation accuracies.

\subsection{Acquisition Function}
Now we explore the impact of acquisition functions for \gls{scp} using the best search and calibration settings reported in Section \ref{sec: sc_setting}. Figure \ref{fig: scp_acq_func} displays the empirical results for the four acquisition functions that are described in Section \ref{subsec:acq_func}. 

\vspace{0.5em}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35 ]{figs/scp_acq_func.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Impact of Acquisition Function on SCP Performance}
	\label{fig: scp_acq_func}
	\\
 	\parbox{\linewidth}{
	\vspace{1em}
 	  	{\small \textit{Figure \ref{fig: scp_acq_func}:} Performances of SCP using various acquisition functions, with 10 initial evaluations and calibration set containing 50\% of all data points.
 	  	}}
 	}
\end{figure}

In line with the findings for BANANAS \cite{white2019bananas}, the choice of acquisition function appears to have little to no effect on performances for SCP. For each of the datasets, all methods result in comparable validation accuracies and \gls{rmsce}, with the error bands around the average value significantly overlapped and indistinguishable throughout the search time. Moreover, no single acquisition function dominates across different datasets: when looking at the final validation accuracy, \gls{its} and \gls{pi} perform a bit better than the others on CIFAR10, while \gls{pi} and \gls{ucb} are slightly ahead on CIFAR100 and ImageNet16-120, respectively. Although the effect of acquisition function on mean validation accuracy is minor, introducing randomness into a mutation-based search process appears to reduce performance variability. Among these acquisition functions, \gls{its} is the most stochastic one due to its sampling-based nature and exhibits the smallest standard deviation on all datasets. 

\subsection{Acquisition Search Strategy}
Next, we explore different acquisition search strategies that are described in Section \ref{subsec:acq_strats}, namely \textit{mutation}, \textit{dynamic}, and \textit{random sampling}, ordered in increasing levels of randomness. Since former experiments indicates that the choice of acquisition function has little impact for \gls{scp}, to ensure consistent comparison with the pure BANANAS method, we adopt \gls{its} for the acquisition function when experimenting with various sampling approaches.

Figure \ref{fig: scp_acq_search_strategy.png} reflects that acquisition search strategy has an impact on both uncertainty estimation and final performance for \gls{scp}. The mutation-based approach consistently delivers the best performance across datasets, achieving slightly higher and more stable final validation accuracies. Notably, when the level of randomness increases, the methods tend to end up with lower average performance and higher variability. We repeat the same experiments using the original BANANAS approach (Appendix \ref{appendix: s2}). While mutation- and dynamic-based methods exhibit similar behaviors, random-sampling consistent underperforms the others across datasets.
\vspace{0.3em}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35 ]{figs/scp_acq_search_strategy.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Impact of Acquisition Search Strategy on SCP Performance}
	\label{fig: scp_acq_search_strategy.png}
	\\
 	\parbox{\linewidth}{
	\vspace{1em}
 	  	{\small \textit{Figure \ref{fig: scp_acq_search_strategy.png}:} Performances of SCP using various acquisition search strategy, with 10 initial evaluations and calibration set containing 50\% of all data points.}}
 	}
\end{figure}

Recall from the previous sub-section that \gls{its} in a mutation-based strategy gives the most stable performance. These observations suggest that while a certain degree of randomness seems to be beneficial, excessive randomness may hinder convergence and lead to inconsistent search performances. Likewise, prior work that evaluates Bayesian optimization in a low budget setting shows that when the search budget is low, local search is favored since an optimization method cannot sample the global search space sufficiently \cite{nomura2019simple}.

In general, the baseline strategy, \gls{scp} with ensemble predictor, fails to outperform the original BANANAS method. This is likely due to the limited data available for both training and calibration. Under the current settings for search and calibration, each set only has around 75 data points even by the end of the search, which may limit both prediction accuracy and calibration quality. 

\section{Methods Based on Ensemble Predictor}
In this section, we continue using an ensemble predictor as the underlying prediction algorithm and conduct experiments for methods based on \gls{cvcp} and \gls{btcp}, which enable more efficient use of data by leveraging the entire dataset for both training and calibration.


\begin{description}[leftmargin=0cm, listparindent=\parindent]
 	\item[Performance for CrossVal-CP:] We apply a 3-fold cross-validation and evaluate using \gls{its} as acquisition function. However, the results are somewhat counterintuitive. 

	\vspace{0.3em}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35 ]{figs/cvcp.png}
		\refstepcounter{figure}
   		\addcontentsline{lof}{figure}{Figure~\thefigure: Performances of \gls{cvcp} with Ensemble Predictor under 3-fold Cross-Validation}
		\label{fig: cvcp}
		\\
 		\parbox{\linewidth}{
		\vspace{1em}
 	  		{\small \textit{Figure \ref{fig: cvcp}:} Performance comparisons of various search algorithms: BANANAS, SCP with calibration ratio of 0.5, and CrossVal-CP under 3-fold cross validation.}
 	 	}	
	\end{figure}
	
	The \gls{rmsce} in Figure \ref{fig: cvcp} (top) reflects that \gls{scp} dominates over \gls{cvcp} and consistently attains lower calibration error throughout the search time. As a result, \gls{cvcp} yields the weakest performance in uncertainty estimation among all evaluated approaches. Similar to previous findings with \gls{scp}, calibration error does not show strong correlation with the search performance. Uncalibrated BANANAS remains to be the top-performing method and result in highest mean final validation accuracy. Moreover, cross-validation appears to introduce a rise in the standard deviation of final validation accuracies, especially when compared with \gls{scp}, reflecting reduced model stability across runs. 
	
	\item[Performance for Bootstrap-CP:] An important engineering design within the \gls{btcp} framework is the number of bootstrapped samples. \cite{pmlr-v139-xu21h} suggests that conformalization with 20 to 30 bootstrapped samples is sufficient to yield good and stable outputs. However, considering the limited data availability and potential computation time, we restrict our experiments to 5 and 10 bootstrap samples. The reason behind is that \gls{btcp} methods using 5 bootstrap samples can be viewed as a fair comparison to approaches employing an ensemble of 5 \gls{fnns}, while \gls{btcp} with 10 bootstraps aligns with methods based on a quantile regressor with 10 quantile levels. 
	
		All ensemble-based approaches discussed so far employ a normalized absolute residual for conformity function, as specified by Equation \ref{cpscore}. In contrast, \gls{btcp} leverages a pure absolute residual \footnote{$|y_i - \hat{y}(x_i)|$} that disregards standard deviation estimate, because within a setting with only either 5 or 10 bootstraps, sub-samples may not have sufficient observations for variance estimation and may thereby introduce biases. Nevertheless, we have run experiments using this regular conformity score function (Appendix \ref{appendix: s3}). The empirical results support our prior expectations and show that including standard deviation deteriorates performance.
	
	\vspace{0.3em}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.35 ]{figs/bootstrap.png}
		\refstepcounter{figure}
   		\addcontentsline{lof}{figure}{Figure~\thefigure: Performances of \gls{btcp} with Ensemble Predictor}
		\label{fig: btcp}
		\\
 		\parbox{\linewidth}{
		\vspace{1em}
 	  		{\small \textit{Figure \ref{fig: btcp}:} Performance comparisons of various search algorithms: BANANAS, SCP with calibration ratio of 0.5, and \gls{btcp} with 5 and 10 bootstraps.}
 	 	}	
	\end{figure}	 
	
	Results displayed in Figure \ref{fig: btcp} demonstrate a similar pattern as the previous findings. In terms of the validation accuracy, BANANAS still consistently outperform all methods leveraging calibration techniques across datasets. \gls{scp} outperforms \gls{btcp} based approaches, offering the best and most stable final validation accuracy. For \gls{btcp}, the effect of increasing the number of bootstrap samples is unclear, as the differences in performance between the two variants are minimal and insignificant. Although the experiment results are somewhat disappointing, an interesting observation is \gls{btcp} with more bootstrap samples exhibits higher \gls{rmsce} and achieves poorer calibration outputs, indicating that more sophisticated calibration operations via CP may introduce extra noises into the model.
	
	\item[Prediction Error Analysis:] Overall, the original BANANAS outperforms all ensemble-based calibrated search methods. In contrast to our prior expectation, SCP distinguishes as the best choice for performing calibration in this setting, while alternative methods that support more efficient data use	in general underperform in terms of both \gls{rmsce} and validation accuracies. The decline in performance may stem from various factors, possibly arising   either during the model training phase or in the calibration process. While the latter is difficult to verify, it is straitforward to perform a sanity check on the training performance. To do this, we compute the absolute prediction error across the search duration for a selected subset of evaluated methods that give best performance in their respective group, and compare the prediction accuracies. Figure \ref{fig: pred_err} visualizes models' prediction errors over time, where mean and standard deviation are aggregated over epochs and experiment trials. Notably, since we follow the setup of \cite{white2019bananas} and output 10 architectures in parallel, the prediction errors are smoothed using a rolling window of size 10.
	
		\vspace{0.7em}
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.387 ]{figs/prediction_error_ensemble.png}
			\refstepcounter{figure}
   			\addcontentsline{lof}{figure}{Figure~\thefigure: Prediction Error over Search Time for Ensemble-based Methods}
			\label{fig: pred_err}
			\\
 			\parbox{\linewidth}{
			\vspace{1em}
 	  			{\small \textit{Figure \ref{fig: pred_err}:} Absolute prediction error over epochs of different ensemble-based search methods: BANANAS, SCP with calibration ratio of 0.5, \gls{cvcp} with 3-fold cross-validation, and \gls{btcp} with 5 bootstraps.}
 	 		}	
		\end{figure}	

		Based on Figure \ref{fig: pred_err}, all methods leveraging CP-based calibration techniques do not demonstrate significantly different prediction behaviors, with \gls{btcp} showing a slight edge. Meanwhile, it is noticeable that BANANAS consistently achieves a substantial advantage in prediction, which may highlight one potential source of BANANAS' superior performance.
		
\end{description} 

\section{Methods Based on Quantile Regressor}
In this section, we present the empirical performance of approaches using a quantile regressor for prediction.Specifically, we apply quantile regression in combination with two different \gls{cp} techniques: \gls{scp} and \gls{cvcp}.

\begin{description}[leftmargin=0cm, listparindent=\parindent]
 	\item[Performance for SCP:] We adopt the best settings 
 	
 	\item[Performance for \gls{cvcp}:]
 	
\end{description}


\section{Diagnosis Analysis}



% chapter 6
\chapter{Conclusion}
This chapter presents the central findings of this work as well as their critical discussions (Section \ref{sec: discussion}). Finally, it highlights limitations and corresponding opportunities for further research (Section \ref{sec: future_work}).

\section{Discussion}
\label{sec: discussion}

\section{Limitations and Future Work}
\label{sec: future_work}


% Reference
\bibliographystyle{plain}
\bibliography{references}

% Appendix
% List of algorithms/figures/tables
\listofalgorithms 
\listoffigures 
\listoftables
\printglossary[type=\acronymtype, title=Acronyms]

\appendix
\chapter{Program Code and Data Resources}
The source code and a documentation are available at the GitHub repository: \url{https://github.com/chengc823/Thesis.}
The datasets used for experiments and algorithm evaluations are sourced from the \href{https://github.com/automl/NASLib/tree/Develop}{NASLib repository}.

In case of access or permission issues to the private repository, please reach out at: chechen@mail.uni-mannheim.de.

\chapter{Additional Experimental Results}
\section{Number of Quantiles}
\label{appendix: s1}
\section{Acquisition Search Strategies}
\label{appendix: s2}
\section{Conformity Score Functions}
\label{appendix: s3}


% Declare non-Plagiarism and independent work
\backmatter
\chapter{Ehrenwörtliche Erklärung}
Ich versichere, dass ich die beiliegende Bachelor-, Master-, Seminar-, oder
Projektarbeit ohne Hilfe Dritter und ohne Benutzung anderer als der angegebenen
Quellen und in der untenstehenden Tabelle angegebenen Hilfsmittel angefertigt
und die den benutzten Quellen wörtlich oder inhaltlich entnommenen Stellen als
solche kenntlich gemacht habe. Diese Arbeit hat in gleicher oder ähnlicher Form
noch keiner Prüfungsbehörde vorgelegen. Ich bin mir bewusst, dass eine falsche
Erklärung rechtliche Folgen haben wird.

% Declare the use of AI tools.
\vspace{0.5cm}
  \textbf{Declaration of Used AI Tools} \\[.3em]
  \begin{tabularx}{\textwidth}{lXlc}
    \toprule
    Tool & Purpose & Where? & Useful? \\
    \midrule
    ChatGPT & Rephrasing & Throughout & + \\
    ChatGPT & Debugging LaTeX syntax errors  & Equation & + \\
    ChatGPT & Rendering LaTeX tables from Python frame & Tables & + \\
    \bottomrule
  \end{tabularx}
\end{center}

\vspace{2cm}
\noindent Unterschrift\\
\noindent Mannheim, den 30.07.2025 \hfill

\end{document}
