% Do not change document class, margins, fonts, etc.
\documentclass[a4paper,oneside,bibliography=totoc]{scrbook}
\setlength{\parindent}{25pt}
\newtheorem{definition}{Definition} \newtheorem{proposition}{Proposition}
\usepackage{appendix}

% Add packages
\usepackage[
colorlinks=true, urlcolor=blue, linkcolor=black, colorlinks,citecolor=green
]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{emptypage}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[justification=raggedright]{caption}
\captionsetup{labelformat=empty, textformat=empty}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{algorithm} % you can modify the algorithm style to your liking
\usepackage{algorithmic}
\usepackage{csquotes}
\renewcommand{\algorithmiccomment}[1]{\hfill\textit{// #1}}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[nopostdot, style=super, nogroupskip, nonumberlist, toc]{glossaries}
\makeglossaries

% Citation style
\usepackage{cite
\usepackage{csquotes}
\bibliographystyle{chicagoa}
\setcitestyle{authoryear,round,semicolon,aysep={},yysep={,}} \let\cite\citep

%---------------------------------------------------------------------------------------
\input{acronyms}

% Begin Documents
\begin{document}
\setlength{\skip\footins}{20pt}
% Cover page
\frontmatter \subject{Master Thesis} % change to appropriate type
\title{\LARGE 
	Uncertainty Calibration with Online Conformal Prediction in Neural Architecture Search: \\ 
	An Evaluation under the BANANAS Framework 
}
\author{
	Cheng Chen\\ (matriculation number 1662473)} \date{July 31, 2025
}
\publishers{
	{\small Submitted to}\\
	Data and Web Science Group\\Prof.\ Dr.\ Margret Keuper\\University of Mannheim\\
}
\maketitle

% Abstract
\chapter{Abstract}
Some contents


% Table of contents
\begingroup%
\hypersetup{hidelinks} % disable link color in TOC only
\tableofcontents%
\endgroup


% Body
\mainmatter  % start new numbering

% chapter 1
\chapter{Introduction}
\gls{bananascp} \\
\gls{bananas} \\
\gls{nas} \\
\gls{cp}
\gls{scp}
\gls{cvcp}
\gls{btcp} 

\label{ch:intro}

\section{Related Work}
\section{Contributions and Limitations}
\section{Outline}
Having gained an overview of the research question and the background, the remainder of this thesis is organized as follows. First, Chapter 2 reviews the related works on neural architecture search, uncertainty quantification, and in particular, conformal prediction. In Chapter 3, after proposing a novel framework to incorporate uncertainty calibration into the architecture search process in Section \ref{sec:overview}, we describe its methodological steps in more detail. In Section \ref{sec:cp}, we identify different types of conformal prediction algorithms that are applicable for \gls{nas}, and consider the use of the underlying surrogate models. In Section \ref{sec:distest} and Section \ref{sec:acq}, we further examine how the calibrated predictions can be incorporated into a Bayesian optimization process. In Chapter 4, we present an overview of the general experiment setups and the strategy for progressively tuning configurations, along with a description of the benchmark dataset used for research. In Chapter 5, we present the experimental results and compare the performance of the algorithms with state-of-the-art methods. Finally, Chapter 6 and Chapter 7 conclude this work and discuss potential future directions.

% chapter 2
\include{background}

% chapter 3
\include{methodology}

% chapter 4
\chapter{Experiment Design}
\label{ch4}
To compare the performance of BANANAS--CP with the original BANANAS method and assess the role of uncertainty calibration, we choose the widely used tabular benchmark dataset NAS-Bench-201 \cite{dong2020nasbench201} for experiments. In this chapter, we first provide a description of the dataset (Section \ref{sec: dataset}). Then, we introduce the general setups that are shared across all experiments, along with the strategy for step-wise configuration tuning  (Section \ref{sec: setups}).

\section{Dataset}
\label{sec: dataset}
NAS-Bench-201 is a cell-based architecture search space. Each cell is expressed as a densely connected \gls{dag} with in total 4 nodes and 6 edges. The nodes within a cell represents the sum of all feature maps transformed through the associated operations of the edges pointing to this node, and the edges represent the architectures operation that are chosen from the predefined operation set. Specifically, the operation set comprises 5 representative operations: (1) zeroize, (2) skip connection, (3) 1-by-1 convolution, (4) 3-by-3 convolution, and (5) 3-by-3 average pooling layer. This search space contains all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 cell candidates in total. The macro structure of an architecture is defined as a chain of blocks, which is initiated with one 3-by-3 convolution with 16 output channels and a batch normalization layer, and consists of three stacks of cells that are connected by a residual layer. Figure \ref{fig: nasbench201} illustartes the structure of an architecture in this search space.  
	\vspace{0.5em}	
	\begin{figure}[bthp]
		\centering
		\includegraphics[scale=0.48]{figs/nas_bench_201.png}
		\refstepcounter{figure}
   		\addcontentsline{lof}{figure}{Figure~\thefigure: Illustration of the overall network architecture structure in NAS-Bench-201}
		\label{fig: nasbench201}
			\parbox{\linewidth}{
	 		\vspace{0.7em}
 	 		{\small \textit{Figure \ref{fig: nasbench201}:} Illustration of the skeleton (top) and the design of individual cells (bottom) of architectures in NAS-Bench-201 \cite{dong2020nasbench201}.
 	 		}
 		}
	\end{figure}
\newline

\newline
Architectures in the search space are evaluated on three datasets that are widely used for image classification tasks: CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning} and ImageNet-16-120 \cite{chrabaszcz2017downsampled}. Each dataset is split into the training, validation, and test sets using a standard evaluation pipeline. NAS-Bench-201 provides the training, validation, and test losses as well as accuracies for all architectures in the search space. The following gives a brief introduction to these datasets:

\begin{description}[leftmargin=0cm, listparindent=\parindent]
 	\item[CIFAR10]:	The dataset consists of 60K $32\times32$ color images in 10 classes. In NAS-Bench-201, 25K images with 10 classes are assigned into the training and the validation sets, respectively. The test set contains 10K images, with 1K images per class.
 	\item[CIFAR100]: This dataset has the same images as CIFAR-10 but in 100 classes. The training set has 50K images, and each of the validation and the test sets has 5K images.
 	\item[ImageNet-16-120]: This dataset contains 151.7K training images, 3K validation images, and 3K test images with 120 classes. Each image has 16$\times$16 pixels.
\end{description} 


Since its introduction, NAS-Bench-201 has contributed to the \gls{nas} community in several aspects. First, it provides full training and test results (e.g., accuracy, training time, etc.) for every possible architecture in the space on three datasets, allowing reproducible NAS experiments without training models from scratch. Because all architectures are pre-evaluated, \gls{nas} methods can be benchmarked extremely efficiently and subsequent \gls{nas} research can just focus on the search algorithms without any model evaluation. Moreover, using a unified dataset splitting strategy, NAS-Bench-201 reduces the variability caused by different implementation details or training setups, which is a limitation of previous benchmarks, and thus enables consistent comparisons across different \gls{nas} algorithms. 

\vspace{0.2em}	
NAS-Bench-201 also serves as a foundation for extending benchmark datasets. In particualr, \cite{jung2023neural} evaluates all 6,466 non-isomorphic architectures in the space for robustness against adversarial attacks and common image corruptions, and introduces a dataset that includes both clean and robust accuracy values. This dataset covers adversarial attacks and corruptions of different severity levels, enabling a systematical study on how architectural variations impact robustness.

\vspace{0.2em}
In this work, we leverage the API offered by NAS-Bench-201 and directly query the pre-computed train and test metrics of architectures \footnote{Performance metrics for training and testing on the three datasets are downloaded from NASLib: \ulr{https://github.com/automl/NASLib/tree/Develop}}. Notably, NAS-Bench-201 also provides several analytical metrics, such as model rankings and accuracy correlations across the three datasets, which further guide our post-hoc analysis of the experimental results.


\section{Setups and Implementation}
\label{sec: setups}
We now present the common experimental setups used throughout this study. As introduced in Chapter \ref{ch3}, we primarily run experiments for five calibrated NAS algorithms varying in calibration techniques and/or surrogate predictors: \gls{scp} with ensemble predictor, \gls{scp} with quantile regressor, \gls{cvcp} with ensemble predictor, \gls{cvcp} with quantile regressor, and \gls{btcp} with ensemble predictor. In line with \cite{white2019bananas}, each algorithm is given a search budget of 150 epochs to ensure consistent benchmarking with \gls{bananas}. As in previous works, the validation accuracy is used as the supervision signal to guide the search. Each algorithm is repeated for 50 trials with different random seeds and the final results are obtained by aggregating the performance across all trials. 

\vspace{0.2em}
Although training neural networks is avoided thanks to the benchmark datset, each search algorithm still involves a large number of hyperparameters, making it unrealistic to tune them all. Also, not all hyperparameters have the same impact on search performance; some might be more important than the others. Therefore, we select a subset of hyperparameters that we believe, either base on experience or preliminary testing results, are less important or already well-set, and fix their values throughout the experiments. For instance, we believe path-encoding is stronger than other architecture encoding techniques, thus architectures are always encoded using paths present in the cell in all experiments. As for the acquisition optimization strategy, the number of candidates that the acquisition function evaluates in each iteration is fixed at 100 and the maximal mutation allowed for each model is  1 in case the mutation strategy is adopted. In addition, we follow \cite{white2019bananas} and output 10 architectures to mimic the parallelized evaluation procedure.
\vspace{0.2em}

The hyperparameters are tuned progressively. In the first stage, we focus on configurations common to all algorithms. We start by conducting a thorough analysis on the baseline method, i.e., \gls{scp} with ensemble predictor, to find the optimal general setting, like the number of quantiles, the size of the initial dataset (the number of model evaluations before fitting the surrogate), the acquisition functions and the sampling strategies, etc. This optimal setting will be applied to other more advanced approaches in the next stage. Then, we turn to hyperparameter specific to each search algorithm and 
	conduct separate experiments that are  discussed in the respective sections.

\vspace{0.2em}
We borrow the implementation of \gls{bananas} from NASLib \footnote{https://github.com/automl/NASLib}, which is a modern Python-based framework for \gls{nas} developed by the AutoML Freiburg group. NASLib is well modularized, enabling a relatively easy integration of the new calibration block. In NASLib, each \gls{nas} algorithm typically comprises a \textit{trainer} for initiating search and evaluation iterations and an \textit{optimizer} for encapsulating specific search logics, including an inherent \textit{predictor} if applicable. Specifically, \textit{trainer} serves as a generic engine and is shared across all NAS algorithms. All predictors should conform to a particular interface so that they can be invoked inside the \textit{trainer}. 

\vspace{0.2em}
Building on this structure, we add new modules for constructing the \gls{bananascp} framework, including a quantile regressor, distribution estimators (along with the compatible acquisition functions), and the calibration algorithms. We leverage the existing implementation for \textit{trainer} with mild modifications on the export functionalities, aiming for a better access to intermediate outputs, such as the estimated distribution at each iteration. In addition, we also provide tools for analyzing and interpreting the experimental results.

% chapter 5
\chapter{Results}
This chapter presents the performance of the methods introduced in Chapter \ref{ch3}, evaluated using the experimental setups described in Chapter \ref{ch4}. 

\section{Baseline}
\label{sec: baseline}

\subsection{Search and Calibration Setting}
\label{sec: sc_setting}
To establish baseline performance, we run experiments for both the original \gls{bananas} method and the approach incorporating \gls{scp} with an ensemble predictor on all three datasets. In this sub-section, we explore general search and calibration settings to gain a first insight of the performance of \gls{bananascp}.

For the pure \gls{bananas}, we follow the best settings reported in \cite{white2019bananas}, using an ensemble of 5 \gls{fnns}, along with  \gls{its} and a mutation strategy that mutates the top 2 performing architectures for computing acquisition scores. For the variant that incorporates SCP calibration, we evaluate under different configurations: number of the initial model evaluations (either 10 or 30), split rate representing the fraction of data points assigned into the calibration set (either 0.3 or 0.5), and number of quantiles (10). For example, specifying 20 quantile levels yields a percentile vector with 20 values (0, 0.05, 0.1, ..., 0.95, 1). After applying a filtering step to remove extreme percentiles, the remaining values are used as mis-coverage rates when performing conformal prediction, and the resulting calibrated quantiles form the basis for a linearly-interpolated distribution. Unless otherwise specified, 10 quantiles are used throughout the experiments, since results on both the synthetic dataset (Table \ref{table:distest}) and CIFAR10 (Appendix \ref{appendix: b}) indicate that 10 quantiles yield more robust estimates than 20 when the dataset is relatively small with up to 150 data points.

Figure \ref{} presents the search performances for strategies with different initial dataset sizes (top) and train-calibration splitting ratios (bottom). Search performance, measured by validation accuracy of the best architecture found so far, and calibration error \gls{rmsce} are reported at each epoch. Specifically, the line denotes the average over 50 trials, and the surrounding band represents the variation, corresponding to one standard deviation. 


\subsection{Acquisition Function}
Next, we explore the impact of acquisition functions using the best search and calibration setting found in Section \ref{sec: sc_setting}.


\subsection{Acquisition Search Strategy}
Experiments on the acquisition function presented in the previous sub-section su


% chapter 6
\chapter{Conclusion}
This chapter presents the central findings of this work as well as their critical discussions (Section \ref{sec: discussion}). Finally, it highlights limitations and corresponding opportunities for further research (Section \ref{sec: future_work}).

\section{Discussion}
\label{sec: discussion}

\section{Limitations and Future Work}
\label{sec: future_work}


% Reference
\bibliographystyle{plain}
\bibliography{references}

% Appendix
% List of algorithms/figures/tables
\listofalgorithms 
\listoffigures 
\listoftables
\printglossary[type=\acronymtype, title=Acronyms]

\appendix
\chapter{Program Code and Data Resources}
The source code and a documentation are available at the GitHub repository: \url{https://github.com/chengc823/Thesis.}
The datasets used for experiments and algorithm evaluations are sourced from the \href{https://github.com/automl/NASLib/tree/Develop}{NASLib repository}.

In case of access or permission issues to the private repository, please reach out at: chechen@mail.uni-mannheim.de.

\chapter{Additional Experimental Results}
\label{appendix: b}


% Declare non-Plagiarism and independent work
\backmatter
\chapter{Ehrenwörtliche Erklärung}
Ich versichere, dass ich die beiliegende Bachelor-, Master-, Seminar-, oder
Projektarbeit ohne Hilfe Dritter und ohne Benutzung anderer als der angegebenen
Quellen und in der untenstehenden Tabelle angegebenen Hilfsmittel angefertigt
und die den benutzten Quellen wörtlich oder inhaltlich entnommenen Stellen als
solche kenntlich gemacht habe. Diese Arbeit hat in gleicher oder ähnlicher Form
noch keiner Prüfungsbehörde vorgelegen. Ich bin mir bewusst, dass eine falsche
Erklärung rechtliche Folgen haben wird.

% Declare the use of AI tools.
\vspace{0.5cm}
  \textbf{Declaration of Used AI Tools} \\[.3em]
  \begin{tabularx}{\textwidth}{lXlc}
    \toprule
    Tool & Purpose & Where? & Useful? \\
    \midrule
    ChatGPT & Rephrasing & Throughout & + \\
    ChatGPT & Debugging LaTeX syntax errors  & Equation & + \\
    ChatGPT & Rendering LaTeX tables from Python frame & Tables & + \\
    \bottomrule
  \end{tabularx}
\end{center}

\vspace{2cm}
\noindent Unterschrift\\
\noindent Mannheim, den 30.07.2025 \hfill

\end{document}
