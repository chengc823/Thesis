% Do not change document class, margins, fonts, etc.
\documentclass[a4paper,oneside,bibliography=totoc]{scrbook}
\setlength{\parindent}{25pt}
\newtheorem{definition}{Definition} \newtheorem{proposition}{Proposition}
\usepackage{appendix}

% Add packages
\usepackage[
colorlinks=true, urlcolor=blue, linkcolor=black, colorlinks,citecolor=green
]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{emptypage}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[justification=raggedright]{caption}
\captionsetup{labelformat=empty, textformat=empty}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{algorithm} % you can modify the algorithm style to your liking
\usepackage{algorithmic}
\usepackage{csquotes}
\renewcommand{\algorithmiccomment}[1]{\hfill\textit{// #1}}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[nopostdot, style=super, nogroupskip, nonumberlist, toc]{glossaries}
\makeglossaries

% Citation style
\usepackage{cite
\usepackage{csquotes}
\bibliographystyle{chicagoa}
\setcitestyle{authoryear,round,semicolon,aysep={},yysep={,}} \let\cite\citep

%---------------------------------------------------------------------------------------
\input{acronyms}

% Begin Documents
\begin{document}
\setlength{\skip\footins}{20pt}
% Cover page
\frontmatter \subject{Master Thesis} % change to appropriate type
\title{\LARGE 
	Uncertainty Calibration with Online Conformal Prediction in Neural Architecture Search: \\ 
	An Evaluation under the BANANAS Framework 
}
\author{
	Cheng Chen\\ (matriculation number 1662473)} \date{July 31, 2025
}
\publishers{
	{\small Submitted to}\\
	Data and Web Science Group\\Prof.\ Dr.\ Margret Keuper\\University of Mannheim\\
}
\maketitle

% Abstract
\chapter{Abstract}
Some contents


% Table of contents
\begingroup%
\hypersetup{hidelinks} % disable link color in TOC only
\tableofcontents%
\endgroup


% Body
\mainmatter  % start new numbering

% chapter 1
\chapter{Introduction}
\gls{bananascp} \\
\gls{bananas} \\
\gls{nas} \\
\gls{cp}
\gls{scp}
\gls{cvcp}
\gls{btcp} 

\label{ch:intro}

\section{Related Work}
\section{Contributions and Limitations}
\section{Outline}
Having gained an overview of the research question and the background, the remainder of this thesis is organized as follows. First, Chapter 2 reviews the related works on neural architecture search, uncertainty quantification, and in particular, conformal prediction. In Chapter 3, after proposing a novel framework to incorporate uncertainty calibration into the architecture search process in Section \ref{sec:overview}, we describe its methodological steps in more detail. In Section \ref{sec:cp}, we identify different types of conformal prediction algorithms that are applicable for \gls{nas}, and consider the use of the underlying surrogate models. In Section \ref{sec:distest} and Section \ref{sec:acq}, we further examine how the calibrated predictions can be applied in a Bayesian optimization process. In Chapter 4, we describe the benchmark dataset for conducting experiments. In Chapter 5, we present the experiment setups and  compare the algorithm performances against the state-of-the-art techniques. We also  provide interpretations of the results. Finally, Chapter 6 and Chapter 7 conclude this work and discuss potential future directions.


% chapter 2
\chapter{Background}
This chapter offers the technical background related to the research question of this work. We start by providing a comprehensive overview of \gls{nas} and introduce the three dimensions that characterize a \gls{nas} algorithm, followed by an anatomy of the high-performing search algorithm \gls{bananas}. Then, we review the existing uncertainty quantification techniques, with a focus on \gls{cp} algorithms, particularly those related to the novel framework we propose in Chapter~\ref{ch3}.

\section{Neural Architecture Search}
\subsection{Overview}
\label{sec: nas}
In the recent decades, deep learning has achieved remarkable success in a variety of areas, including computer vision, natural language understanding, and machine translation. This success is partly attributed to the meticulously hand-crafted neural network architectures. With the rising demand for efficient architecture engineering in complex domains, \gls{nas} has emerged as a technique for automating the design of neural architectures for specific tasks.
 
\gls{nas} has been a rapidly progressing research domain in the past years. Since the seminal work that achieves competitive performance on CIFAR-10 \cite{zoph2017neural}, numerous \gls{nas} algorithms built on different techniques have been proposed. In general, \gls{nas} algorithms can be characterized by three key dimensions: search space, search strategy, and performance evaluation strategy \cite{elsken2019neural, white2023survey, wistuba2019survey}. Figure \ref{fig: nas_overview} illustrates a typical architetcture search process.


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{figs/overview_nas.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Overview of Neural Architecture Search}
	\label{fig: nas_overview}
	\parbox{\linewidth}{
	 \vspace{0.5em}
 	 {\small \textit{Figure \ref{fig: nas_overview}:} Overview of an architecture search process. The search strategy iteratively selects architectures from a predefined search space $\mathcal{A}$. The performance estimation strategy evaluates the model performance on the target dataset and returns the performance to the search strategy.
	}
 	}
\end{figure}

\noindent Next, we provide definitions of the terms and review the research progress of each domain.
\begin{description}[leftmargin=0cm, listparindent=\parindent]
	\item [Search Space] A search space defines a set of architectures that the search algorithm is allowed to select. The search space is often the first step when setting up \gls{nas} and perhaps is also the most essential step, because
		the design of the search space represents an important trade-off between human bias and efficiency of search: a
		smaller search space incorporating more prior human knowledge and involving more manual decisions will enable
		\gls{nas} algorithms to find high-performing architectures more easily, in contrast a larger space with more primitive
		building blocks provides higher odds of discovering truly novel architectures \cite{white2023survey}. Common  search
		spaces range in size from a few thousand to over $10^{20}$.	
	
		There are four major categories of search spaces in the \gls{nas} literature \cite{white2023survey}. We start with two types of search spaces that have relatively simple architecture topologies. The macro search spaces \cite{baker2017designing, kandasamy2018neural, zoph2017neural} encode the entire neural architecture at a high level. Typically, an entire architecture is often represented by a \gls{dag}, with nodes defining the operation types and edges representing data flows. Each node is allowed to have distinct structures, such as convolution, pooling. As a result, macro search  spaces are highly flexible and possess high representation power. Another type is the chain-structured search spaces. As suggested by the name, chain-structured search spaces consist of neural networks that can be written as a sequence of operation layers. These search spaces often take state-of-the-art manual designs as the backbone. For example, there are several chain-structured search spaces based on the convolutional networks \cite{cai2018proxylessnas} or the transformer architectures \cite{xu2021nasbert}. 
		
		The third group is the cell-based search spaces, which perhaps are the most popular type of search spaces in \gls{nas} research. The cell-based search spaces are inspired by the fact that state-of-the-art human-designed architectures often consist of repeated blocks. For instance, the high-performing Transformer \cite{vaswani2017attention} contains 6 identical stacked encoder and decoder layers. Thus, instead of searching for the entire network architecture from scratch, \cite{zoph2018learning} propose to only search over relatively small cells, and stack the cells according to a predefined skeleton to form the overall architecture. Building on this idea, \cite{zoph2018learning} proposes the first modern cell-based search space, NASNet, which comprises of two types of cells: the normal cell that preserves the dimensionality and the reduction cell that reduces the spatial dimension, as illustrated in Figure \ref{fig: cellss}. Since its emergence, many other cell-based search spaces have been developed. In general, these cell search spaces share a high-level similarity, but differ in the design of the fixed macro structure, the layout and constraints in the cells, and the choices of operations within the cells \cite{dong2020nasbench201, liu2018darts,  pmlr-v97-ying19a}. The cell-based approach significantly reduces the size and the complexity of the search space. However, it has been criticized for limiting the expressiveness of \gls{nas}, potentially hindering the discovery of highly novel architectures \cite{white2023survey}.	
		
		\begin{figure}[htbp]
			\centering
			\includegraphics[scale=0.45]{figs/overview_cell_based_ss.png}
			\refstepcounter{figure}
   			\addcontentsline{lof}{figure}{Figure~\thefigure: Overview of Cell-based Search Space}
			\label{fig: cellss}
			\parbox{\linewidth}{
	 		\vspace{0.5em}
 	 		{\small \textit{Figure \ref{fig: cellss}:} Overview of a cell-based search space NasNet. The outer skeleton across cells (left) is fixed, and the operations, represented by nodes, within the cells are searchable (right) \cite{white2023survey}.
			}
 		}
		\end{figure}
		
		The last main category is the hierarchical search spaces. Different from the aforementioned types of search spaces that mostly have a flat representation, hierarchical search spaces involve designing patterns at different levels, where each higher-level pattern is often represented as a \gls{dag} of lower-level patterns \cite{christoforidis2021novel, liu2018hierarchical}.
		
	In addition to the architecture topology, another important design accompanying a search space is the architecture encodings, because  many \gls{nas} algorithms require representations of the architectures to, for example, mutate an architecture or train a predictive model to extrapolate its performance. For search spaces that can be represented by a \gls{dag}, adjacency matrix is a commonly used encoding method. In addition, other encoding techniques, including graph-based encoding \cite{ning2020generic}, path-based encoding \cite{white2019bananas} and conditionally-structured encoding methods tailored for hierarchical search spaces have been proposed. \cite{white2020study} has shown that the effect of the encoding methods varies across different \gls{nas} subroutines.	
	  
	\item [Search Strategy] According to \cite{white2023survey}, there are generally two main categories of search strategies: black-box optimization based techniques and one-shot techniques. 
	 
		The black-box optimization based techniques largely overlap with another sub-area of AutoML: the hyperparameter tuning. Common techniques for hyperparameter tuning have been proven to be efficient for \gls{nas} as well, including reinforcement learning \cite{zoph2017neural, zoph2018learning}, evolutionary algorithms \cite{maziarz2018evo-nas,  real2019regularized}, gradient descent\cite{liu2018darts}, and etc. In particular, we take a close look at the search strategies based on Bayesian optimization, since they are closely related to the research question of this work. Specifically, initial Bayesian optimization based approaches typically use the Gaussian Process (GP) as the surrogate model \cite{kandasamy2018neural}. However, these algorithms often demonstrate under-performance compared to their competitors due to several limitations: 1) search spaces are usually high-dimensional, non-continuous, and graph-like; 2) GPs requires custom distance metrics among architectures, which involves a time-consuming matrix inversion step. Besides, GPs are difficult to scale since the computation complexity grows cubically with the number of observations. To address these challenges, a new framework that using a neural predictor as the surrogate model for Bayesian optimization has been proposed and demonstrated strong performance \cite{ma2019deep, springenberg2016bayesian, white2019bananas}. We review this framework in details in Section \ref{sec:bananas}.
		
		The one-shot techniques are introduced to avoid training each architecture from scratch, The key idea is to train a \textit{supernetwork} that comprises all possible architectures in the search space as subnetworks. Once a supernet is trained, each architecture from the search space can be evaluated by inheriting the weights from the corresponding subnet within the supernet \cite{pmlr-v80-bender18a, liu2018darts}.
		
	\item [Performance Evaluation] The performance evaluation refers to the process of estimating the performance of architectures. The estimated performance is communicated back to the search algorithm to guide the next search. The simplest performance estimation strategy is to fully train an architecture on the training data and then evaluate its performance on the validation data. However, training each architecture demands substantial computation resources and typically takes serval hours or days on a GPU. Consequently, many methods for speeding up the performance evaluation process for architectures have been proposed. One popular line of work is to predict the performance of neural networks before they are fully trained using the zero-cost proxies \cite{pmlr-v139-mellor21a}.
	
	In this work, we primarily run experiments on the benchmark dataset NAS-Bench-201 \cite{dong2020nasbench201}, which offers queryable validation and test accuracies for all architectures in the search space and thereby eliminates the need to train neural networks when simulating \gls{nas} experiments. Hence, we provide only a brief overview of this aspect and refer the readers to \cite{white2023survey} for a comprehensive introduction to the performance evaluation techniques.	
	\end{description}}



\subsection{Bayesian Optimization and BANANAS}
\label{sec:bananas}
As briefly mentioned in Section \ref{sec: nas}, the Bayesian optimization based \gls{nas} search strategy using a neural network as the surrogate model has shown strong performance. In particular, after identifying five components of this framework, \cite{white2019bananas} performs a thorough analysis on each component's effect towards the search performance and proposes a final algorithm, i.e., \gls{bananas}, based on both theoretical and empirical findings. This method is proven to be efficient, achieving state-of-the-art performance on popular NAS benchmarks 

In this section, we present a detailed review of the work by \cite{white2019bananas}. We start with the theoretical background and give an introduction into the Bayesian optimization method. Next, we walk through the five identified components and provide a summary of the experimental findings.

Bayesian optimization \cite{Mockus1978} is a sequential decision-making process that seeks to find the global maximum (minimum is the negation of the maximum) of an unknown black-box objective function $f : X \rightarrow R$ over an input space $X \subseteq R^D$. In a Bayesian optimization process, the unknown objective function $f$ is treated as a random function and the prior belief over $f$ is encoded by a surrogate model, usually a Gaussian Process or a Parzen-Tree Estimator \cite{bergstra2011algorithms}. At each iteration, the surrogate model updates the prior with the observations and forms a posterior probabilistic distribution of $f$. Then, the acquisition function, another key component that trades off exploration and exploitation in the process, evaluates a set of candidates based on the posterior distribution and picks the data point with the largest acquisition score for next query. Algorithm \ref{alg:BO} outlines this procedure.

\begin{algorithm}[t]
  \caption{Bayesian Optimization}
  \label{alg:BO}
  \begin{algorithmic}[1]
  \textbf{Input:} surrogate model $\mathcal{M}$, acquisition function $\phi$, objective function $\myfunc{f(\cdot)}$, number of iterations $T$. \vskip
  
  \STATE Initialize the set of observations: $D \leftarrow \emptyset$
  \FOR {$t$ in $1,...,T$}
  		\begin{enumerate}
    	    \itemsep0em 
			\item Fit surrogate model $\mathcal{M}$ to current observations set $\mathcal{D}_{t-1}$.
			\item Evaluate acquisition function and select the next point for query: \newline
				  $x_t = \operatorname*{argmax}_{x \in \mathcal{X}} \; \phi(x, \; \mathcal{M})$	
			\item Query the objective function: $y_t = f(x_t)$
			\item Update the observations set: \mathcal{D}_t \leftarrow \mathcal{D}_{t-1} \cup \{(x_t, y_t)\}
		\end{enumerate}
  \ENDFOR
  \STATE \textbf{Output:} $x^*=\operatorname*{argmax}_{t=1,...,T} f(x_{t})$    
 \end{algorithmic}
 \end{algorithm}

The acquisition function adopted in the original paper \cite{Mockus1978} is the \gls{ei}. Other popular alternatives include: \gls{ts}, \gls{its}, \gls{ucb}, and \gls{pi}. Different acquisition functions typically favor exploration and exploitation differently. Nevertheless, \cite{agnihotri2020exploring} shows that \gls{ei} is competitive in reaching the optimum value with comparably few iterations. 

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{figs/overview_bo.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Illustration of Bayesian Optimization}
	\label{fig: bo}
	\parbox{\linewidth}{
	\vspace{0.5em}
 	{\small \textit{Figure \ref{fig: bo}:} Example of Bayesian optimization with Gaussian Process as the surrogate and \gls{ei} as the acquisition function to explore the minimum of the objective function \cite{agnihotri2020exploring}.
 	}
 	}
\end{figure}

Next, we return to the NAS framework "Bayesian optimization + neural predictor". Now it becomes obvious that this framework is essentially an optimization task searching the maximum with a neural network as the surrogate and neural architectures in a search space being the inputs. Specifically, \cite{white2019bananas} identifies five critical components within the framework, which are listed as follows:

\begin{description}[leftmargin=0cm, listparindent=\parindent]
	\item [Architecture Encoding] This item refers to obtaining a vector representation of the architectures. Previous work often encode architectures using an adjacency matrix based approach, where nodes are assigned with an arbitrary ordering  and then binary features for all edges in the \gls{dag} are set to form the final representation.	Notably, the resulting representation of a specific architecture is not deterministic since the encoding  relies on an arbitrary indexing of the nodes. 
 
	In contrast, \cite{white2019bananas} proposes a novel path-based encoding mechanism with optional path truncation. This method simply checks if a path from the input node to the output node, expressed in terms of operations (e.g., input $\rightarrow$ 1$\times$1 conv $\rightarrow$ 3$\times$3 pool$\rightarrow$ output), is present in an architecture. The final encoding is a binary vector indicating which of the possible paths within a cell are present in the architecture. In this way, an architecture is always mapped to the same (though not necessarily unique) path encoding. Experiments show that the path-based encoding substantially increases the performance of neural predictors.
	
	\item [Neural Predictor] This item is about choosing an appropriate neural network for surrogate. A set of neural architectures and their corresponding validation accuracies are randomly sampled from the search space for training and comparing different neural predictors. Among all tested neural predictors, which include VAEs, GCNs, and FNNs with either the adjacency matrix or path-based encoding, FNNs with path encoding demonstrates the strongest performance.
	 
	\item [Uncertainty Estimation] Uncertainty estimates are required to form the probabilistic distribution. For Bayesian neural networks (BNNs), the posterior distribution is inferred over the nework weights. For an ensemble of \gls{fnns}, the distribution is inferred under the Gaussian assumption. The results show that an ensemble of even only 3 to 5 neural networks in general yields more reliable uncertainty estimates than BNNs.
	
	\item [Acquisition Function] In the experiments, five commonly used acquisition functions are examined: \gls{ts}, \gls{its}, \gls{ucb}, \gls{pi}, and \gls{ei}. Each function is adapted to the Gaussian assumption, thereby requiring only the mean and standard deviation estimates to compute the acquisition scores. In the experiments, overall \gls{its} yields the best performance among all the options, although the marginal outperformance is subtle. The results indicate that the acquisition function does not have as significant impact on the search performance as the other examined components in the framework.
		
	\item [Acquisition Optimization] In each iteration of the Bayesian optimization, the goal is to select a candidate from the search space that maximizes the acquisition score. Evaluating the acquisition function for every architecture available in the search space is computationally infeasible, therefore \cite{white2019bananas} proposes to create a set of 100 to 1000 candidates and then choose the architecture with the maximal acquisition score in this set. Specifically, \cite{white2019bananas} explores various approaches for creating this candidate set. The simplest and most natural way is to draw architectures at random. Consider that architectures close in edit distance to those used for training the surrogate model are likely to have more accurate estimates, an alternative is a mutation-based sampling approach, where the candidate set is created via local search by randomly modifying an operation or an edge of the best-performing architectures that have been evaluated so far. In additional, \cite{white2019bananas} also examines a hybrid approach that combines random search with mutation-based search. Their experiments show that the mutation-based approach outperforms its competitors and suggest it is better to search locally rather than globally.
\end{description}}

Finally, the best components found in the aforementioned analyses are transformed into the BANANAS algorithm, which composed of an ensemble of \gls{fnns} using the path encoding, and \gls{its} with a mutation strategy for acquisition (Figure \ref{fig: bananas}). Specifically, uncertainty is estimated based on the Gaussian assumption.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.48]{figs/overview_bananas.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Diagram of the BANANAS framework}
	\label{fig: bananas}
	\vspace{0.5em} 
	\\
 	{\small \textit{Figure \ref{fig: bananas}:} Diagram of the BANANAS framework \cite{white2019bananas}.}}
\end{figure}

\section{Uncertainty Quantification Methods}
While \gls{bananas} employs a relatively simple method for uncertainty estimation, there are many commonly used \gls{uq} techniques that are generally more theoretically grounded and sophisticated. In this section, we review some of these \gls{uq} methods and explain the rationale for choosing \gls{cp} for the development of our approach. Despite not directly tied to this work, this thesis aspires to offer a comprehensive overview of common \gls{uq} techniques applied in deep learning.

\gls{uq} is the science of quantitative characterization and estimation of uncertainties in both computational and real world applications. Formally, the source of uncertainty can be categorized into two types:

\begin{description}[leftmargin=0cm]
\item[Aleatory Uncertainty:] also known as data uncertainty, refers to uncertainty that arises due to inherent noices or randomness in a system and can not be reduced. \item[Epistemic Uncertainty:] also known as model uncertainty, refers to uncertainty that arises due to lack of knowledge, and can be reduced by better modeling or colloecting more data.
\end{description}

A family of methods for quantifying uncertainty directly model the full predictive distribution of the output, i.e., $p(y \mid x)$. One example is the Gaussian Process, which is typically parameterized by a kernel function and the predictive inference can be obtained in an analytical manner based on the observations. Another important example is the Bayesian Neural Network \cite{kendall2017uncertainties, neal1996bayesian}, in which each weight in the network is seen as a random variable rather than a single number. In practice, however, the posterior distributions over weights are intractable due to the integral operation and the high dimensionality. Therefore, the posterior can only be approximated using e.g.,  variational inference or Monte-Carlo sampling methods \cite{neal1996bayesian, springenberg2016bayesian}, both requiring multiple forward passes through the network. 

Like the inference stage in BNNs, some methods also rely on aggregating statistics from sub-networks, such as Monte-Carlo Dropout \cite{gal2016dropout} and Deep Ensemble \cite{lakshminarayanan2017simple}. The idea of Monte-Carlo Dropout is to apply \textit{dropout}, a regularization technique often used for preventing overfitting during the training time by randomly deactivating neurons, also at the inference time. Specifically, the model run multiple times on the same input with stochastically deactivated neurons and thereby get  different predictions, which approximate the distribution of the output. Deep Ensemble involves using a network that outputs two values in the final layer, corresponding to the predicted mean $\mu(x)$ and the variance $\sigma^2(x)$, respectively. This neural network is typically trained by minimizing a custom loss function in which $\mu(x)$ and $\sigma^2(x)$ have opposing effects. To avoid overfitting and lower estimation variance, multiple instances of the neural network are initialized with different weights and trained independently. Then, the final estimates of $\mu(x)$ and $\sigma^2(x)$ are obtained by aggregating the predictions from all these sub-networks. 

Another popular approach is Quantile Regression (QR) \cite{koenker1978regression}. Instead of modeling the full distribution, QR only models a sequence of discrete quantiles of the output. We present QR with more details in Section \ref{sec:cp}. 

However, these \gls{uq} techniques face several challenges in practice. Some methods require multiple forward propagations either during the training or inference time, such as BNNs, Deep Ensemble. This is in general computationally intensive and may cause the models difficult to scale and consequently limit their applications for real-time prediction or in an online setting. In addition, the inference quantile of BNNs depends in if the prior of weights is correctly specified, which demands expertise and can potentially introduce human bias. On the other hand, some models are pre-trained and are only accessible via API, making intervening the training process practically infeasible \cite{mossina2024CVPR}. Moreover, pre-trained models developed using certain datasets may struggle to generalize across different domains or contexts. 

\vspace{0.5cm}

Post-hoc uncertainty calibration techniques can serve as an effective approach in the restricted scenarios described above. Instead of assessing and measuring uncertainties on the model level, calibration targets on adjusting a model's predicted probabilities to make the reported uncertainties better aligned with the actual likelihoods.

Platt Scaling \cite{platt1999probabilistic} probably is the first calibration technique applied in modern machine learning. This method is introduced in the context of Support Vector Machines and is intended for classification problems. For regression problems, one widely used calibration technique is Isotonic Regression \cite{niculescumizil2005predicting}, a technique of fitting a step-wise line to a sequence of observations such that the fitted line is non-decreasing (or non-increasing) everywhere. The monotonicity of the resulting line allows to preserve the order of predicted quantiles, making this method appealing for calibration. For example, \cite{pmlr-v80-kuleshov18a} show that accurate uncertainty estimates can be obtained by training a recalibrator on a holdout dataset using isotonic regression.

Recently, Conformal Prediction (CP) \cite{shafer2008tutorial, vovk2005algorithmic} is emerged as a new, distribution-free framework for uncertainty quantification. This technique has been widely adopted for both classification and regression problems despite of its relatively short history. Fundamentally, CP serves as a model-agnostic wrapper and can be applied with any arbitrary prediction algorithms. It works by constructing a prediction interval (for regression) or a set of possible values (for classification) that will cover the true value with a predefined probability. The fact that \gls{cp} is applied in a post-hoc fashion with minimal assumptions enables it to capture both aleatoric and epistemic uncertainties \cite{mossina2024CVPR}. Moreover, like most other post-hoc calibration methods, \gls{cp} offers the advantage of being light-weighted with little overhead to implement. We present a thorough introduction into the \gls{cp} framework in the following section.

\section{Conformal Prediction}
\label{sec: reviewCP}
\subsection{Theoretical Background}
We start by introducing the concept of \textit{exchangeable data}, which is an key prerequisite for understanding \gls{cp}.
\newline

\noindent
\textbf{Definition (Exchangeability):}  
A sequence of random variables \( (Z_1, Z_2, \dots, Z_n) \) is said to be \textit{exchangeable} if its joint probability distribution is invariant under any permutation of the indices. That is, for every permutation \( \pi \) of the set \( \{1, 2, \dots, n\} \),
\[
P(Z_1, \dots, Z_n) = P(Z_{\pi(1)}, \dots, Z_{\pi(n)})
\]

\noindent Note that exchangeability allows for dependencies among data points, as long as the joint distribution is invariant under any permutation. Accordingly, it is weaker than the i.i.d. assumption. Then, the goal of \gls{cp} can be formally defined as: 
\newline

\noindent
\textbf{Definition (Conformal Prediction):}  Let \(B = \{(x_1, y_1), \dots, (x_n, y_n)\}\) be a bag of observed examples from an exchangeable joint distribution $\mathbb{P}_{X,Y}$, with $x_i \subset R^d$ representing some features and  $Y$ the target variable. Given a new unseen input \(x_{n+1}\), conformal prediction constructs a prediction region \( \mathcal{C}_n^\tau(x_{n+1}) \) using a \textit{conformity score function} such that
\vspace{0.7em}

\begin{equation}
\mathbb{P}\left( y_{n+1} \in \mathcal{C}_n^\tau(x_{n+1}) \right) \geq 1 - \tau
\vspace{1em}
\label{eq: finitecp}
\end{equation}

\noindent
where \( \tau \in (0, 1) \) is the predefined nominal mis-coverage rate. This probability is also known as the \textbf{finite-sample validity} property of \gls{cp} and is taken over the joint distribution of all \(n + 1\) samples. Moreover, the prediction regions for different $\tau$ should be nested. That means, if  $\tau_1 \ge \tau_2$, then $1 - \tau_1$ is a lower confidence level than $1 - \tau_2$ and we have $ \; \mathcal{C}_n^{\tau_1}(x_{n+1}) \subseteq \mathcal{C}_n^{\tau_2}(x_{n+1})$ \cite{shafer2008tutorial}.  

\vspace{0.7em}
\textbf{Conformity score function} is a real-valued function  measures how dissimilar the unseen example $x_{n+1}$ is compared to the existing examples in the bag $B$. Now generalizing this setting to a supervised learning framework, a predictive algorithm $f$ that maps features to the target variable naturally induces a conformity measure. Specifically, we denote $\hat{f}_n$ the predictor trained on the $n$ existing observations in the bag and $d(z_{n+1}, B), \; z_{n+1} = (x_{n+1}, y_{n+1})$ the conformity function, such that a higher value indicates a greater deviation of the new sample from the existing ones. When using $\hat{f}_n$ to generate a point prediction for $x_{n+1}$, since the prediction rule is learnt from examples in the bag, intuitively, the distance between the predicted value $\hat{f}_n(x_{n+1})$ and the true value $y_{n+1}$ informs how different $z_{n+1}$ is compared to the rest examples in the bag. With a conformity score function of this kind, the prediction set is constructed by collecting all values that lie within a certain distance from the true value, i.e., all values such that $d(\cdot, B)$ smaller than a threshold.

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{figs/cp_symmetry.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Illustration of Symmetrical Data}
	\label{fig: cpsymmetry}
	\vspace{0.5em} 
	\\
 	{\small \textit{Figure \ref{fig: cpsymmetry}:} Illustration of Symmetrical Data in Conformal Prediction}}
\end{figure}

\vspace{0.5em} 
An important intuition behind selecting the threshold that eventually defines the prediction region is \textit{symmetry}, that is, all data points are treated symmetrically within the \gls{cp} framework \cite{angelopoulos2023video, angelopoulos2021gentle}. Suppose $d(z_1, B), \, d(z_2, B), \; ... \; ,d(z_n, B)$ is a sequence of observations in the form of conformity scores, the exchangeability assumption implies that the rank of a new data point $d(z_{n+1}, B)$ is uniformly distributed over the observed values. Figure \ref{fig: cpsymmetry} illustrates the concept of symmetry, in which the new data point is equally likely to take any value on the number line. As a result, the $(1 - \tau)$-th quantile of all the conformity scores is a natural answer to achieve the coverage probability stated in Formula \ref{eq: finitecp}. Recall that the coverage probability is achieved on the $n+1$ samples, the quantile value should be adjusted with a finite-sample correction, resulting in a threshold that is exactly equal to 

\begin{equation}
\vspace{0.7em}
\frac{\lceil(1 - \tau)(n + 1)\rceil}{n}\text{-th quantile of conformity scores}
\label{eq:cpquantile}
\vspace{0.7em}
\end{equation}

\noindent
\textbf{Finite-sample Validity} Note that the coverage guarantee provided by the \gls{cp} framework is marginal, meaning it holds on average over all data points. This is a significantly weaker requirement than the conditional coverage, which requires the guarantee to hold for each individual input (Figure \ref{fig: coverage}). Nevertheless, \gls{cp} remains a powerful tool for uncertainty quantification, because the finite-sample coverage is valid regardless of the size of the observations bag or the choice made for the underlying prediction algorithm or the conformity score function. Under the distance interpretation presented earlier, a smaller score indicates that the new sample conforms more closely with the existing observations. In such case, the function is also referred as a negatively-oriented score and, to be precise, it is actually a non-conformity measure. A positively-oriented one also works in practice, provided with correct modifications. 

Although coverage is guaranteed for any conformity scoring function, this scoring method is actually an important engineering decision and has significant impact on the effectiveness of the constructed prediction set. Imagine a classification problem, if we produce a set that contains all possible classes, then surely the true label is covered by the set. However, such prediction set is neither informational nor actionable, offering little value in application. Naturally, this introduces a second evaluation criterion for \gls{cp}: while ensuring coverage guarantees, ideally the yielded prediction set should be as compact as possible.

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.28]{figs/overview_coverage.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Example of Different Coverage Types}
	\label{fig: coverage}
	\vspace{0.5em} 
	\\
 	{\small \textit{Figure \ref{fig: coverage}:} Example of Different Coverage Types}}
\end{figure}

\vspace{1.5em}
\noindent \textbf{A Link to Hypothesis Testing} It is noticeable that \gls{cp} exhibits a couple of structural similarities to hypothesis testing, which is a fundamental tool in statistical inference and perhaps sounds more familiar to most ears. For example, both methods involves producing an interval and relying on a certain threshold to make decisions. In fact, these terms are often used interchangeably in many research work. This motivates us to seek an interpretation of the \gls{cp} framework through the hypothesis testing perspective, such that we gain a deeper and clearer understanding of \gls{cp}. \vspace{2em}

\begin{table}[h]
    \centering
    \refstepcounter{table}
   	\addcontentsline{lot}{table}{Table~\thetable: Comparison between Conformal Prediction and Hypothesis Testing}
   	
 	{\small \textit{Table \ref{tab:cpht_comparison}:} Comparisons between conformal prediction and hypothesis testing}
	\vspace{0.7em}

    \renewcommand{\arraystretch}{1.2}
   	\begin{tabular}{| m{1.5cm} | m{6cm} | m{5.8cm} |}
    \hline
    &\textbf{Conformal Prediction} & \textbf{Hypothesis Testing} \\
    \hline
    \textbf{Input} & nominal mis-coverage level $\tau$ & significance level $\alpha$ \\
    \hline
    \textbf{Decision rule} & compare the conformity score of a data point against the quantile value & compare $p$-value, which is an empirical measure, to the significance level \\ 
    \hline
	\textbf{Output} & prediction interval offering marginal coverage probability $1 - \tau$ & confidence interval that contains the true parameter value with probability ($1 - \alpha$)  \\
	\hline        		
    \end{tabular}
    \label{tab:cpht_comparison}
\end{table}
	 
\vspace{0.5em} 	  	
Both methods take a predefined value ranging from 0 to 1 as input. In the context of \gls{cp}, this value is often interpreted as mis-coverage rate or error rate, referring to the probability that a prediction region fails to cover the true value. The corresponding concept in hypothesis testing is the significance level, referring to the probability of rejecting the null hypothesis when it is actually true. Table \ref{tab:cpht_comparison} offers a summary of the terms used in \gls{cp} alongside with their parallel in hypothesis testing.

Intuitively, we can think \gls{cp} as running a sequence of hypothesis tests for a given new point $x_{n+1}$, with each testing a candidate value for $y_{n+1}$. The hypotheses are as: \\

\vspace{-0.3em}
\indent $H_0$: The new example $(x_{n+1}, y)$ conforms to the existing observations. \\
\indent	$H_1$: The new example $(x_{n+1}, y)$ does not conform to the existing observations. \\
\vspace{-0.3em}

\noindent In this context, the fraction of conformity scores that is larger than that for $(x_{n+1}, y)$, or equivalently, the complement of the quantile of the conformity score for $(x_{n+1}, y)$, can be seen as a $p$-value. If the $p$-value exceeds the predefined significance level, we fail to reject the null hypothesis, and consequently, this candidate $y$ is included in the prediction set. In fact, this interpretation from the hypothesis testing perspective reflects the core idea of the approach known as full conformal prediction, which we formally introduce in the following section.

\subsection{Full Conformal Prediction}
In fact, conformal prediction is originally designed for a transductive setting, in which the prediction for a test point is generated using the entire dataset available at the inference time \cite{gammerman1998learning}. Hence, this transductive \gls{cp} framework is also known as \gls{fcp}. Typically, new observations are revealed sequentially, which means, after the prediction is generated for a test point, its true value is observed before the next prediction is made. 

The core idea for constructing a prediction set for a given test point $x_{n+1}$ is to iterate over all possible values in the target variable space. As performed in hypothesis testing, for each candidate $y$, a conformity score and its corresponding $p$-value will be calculated. Specifically, the conformity score for candidate $\tilde{y}$ is computed in a way where a prediction rule $\hat{f}_{\tilde{y}}$ is learnt using all pairs in the bag \(B = \{(x_1, y_1), \dots, (x_n, y_n), (x_{n+1}, \tilde{y})\}\), then the conformity score is computed as a distance measure between $\hat{f}_{\tilde{y}}(x_{n+1})$ and $\tilde{y}$. This process is repeated for all available candidates and the prediction set is constructed by collecting every $\tilde{y}$ with which the null hypothesis in the previous sub-section can not be rejected. Typically, if the target variable space is continuous, the space is first discretized to form a finite grid, then each element in the set can be checked, e.g., using grid search \cite{angelopoulos2021gentle}.

Moreover, after the true value $y_{n+1}$ is revealed, the underlying bag of observations gets updated by including this data point, and the above procedure needs to start from scratch when generating prediction for each new test point. Suppose the target variable space has a size of $K$, to generate prediction regions for $m$ instances, the underlying prediction algorithm should fit for $K \cdot m$ times. Clearly, \gls{fcp} is in general extremely computationally expensive and renders the application of \gls{fcp} highly unsuitable for training-intensive works, such as with neural networks.

\subsection{Extensions of Conformal Prediction}
Since the transductive version of \gls{cp} that was first proposed in \cite{gammerman1998learning}, several variants of \gls{cp} have been developed with different computational complexities, formal guarantees, and practical applications.

To address the aforementioned inefficient computation problem of \gls{fcp}, Split Conformal Prediction (\gls{scp}), also known as Inductive Conformal Prediction (ICP), was first introduced in \cite{papadopoulos2002inductive} by replacing the transductive inference with inductive inference.  aims to learn a general prediction rule about the data using the observed records. Then, this rule can be applied directly to obtain predictions when new data arrives in sequence, without re-using the training data and retraining the model repeatedly. The main concept involves splitting the data into two non-overlapping subsets, designated for training and calibration, respectively. A predictive model is fit exclusively on the training set, then (non-)conformity scores are computed using the calibration set to obtain the quantile value that determines the width of the prediction interval. Due to its simplicity and computational efficiency, \gls{scp} is one of the most commonly used techniques in the \gls{cp} family. We introduce the methodological steps of \gls{scp} in details with pseudo-code in Section \ref{sec:scp}.

\vspace{0.5em} 	
Apart from hight computational cost, \gls{cp} still faces several challenges in practice: 
\vspace{0.2em} 

\noindent \textbf{Distribution/Covariate Shift:} The finite-sample validity offered by \gls{cp} depends on exchangeable data. However, the key assumption of exchangeability is often violated in real-world applications, in which the underlying data generation process might vary over time. For example, in finance market behavior can shift drastically in response to major world events. \\
 \noindent \textbf{Adaptivity:} The conformity score adopted in the original \gls{scp} work is based on the  absolute residual $|y - \hat{y}|$, which leads to a prediction interval with fixed width and does not adapt to the intrinsic complexity of the specific test example.
  
Various \gls{cp} algorithms have been developed to address limitations and broaden application domains. These extended algorithms can be mainly categorized into three types in accordance with the structural components in a \gls{cp} framework. 

One line of work focus on the coverage rate. Instead of using a static predefined mis-coverage rate $\tau$ for computing the quantile of conformity scores, \cite{gibbs2021adaptive} develops a method called \textit{ACI} where the applied coverage level is dynamically adapted based on empirical mis-coverage frequency of previous prediction sets. This approach is applied in an online setting. At each time $t$, the applied coverage rate $\tau_t$ is derived by decreasing (resp. increasing) the current value if the prediction sets were historically under-covering (resp. over-covering). The amplitude of rate adjustment is jointly controlled by the pre-specified learning rate and the empirical mis-coverage level. As an example, if the preset target coverage rate is 90\%, historical coverage rate suggests the applied coverage should set to 93\%, then this adjusted value is used for computing quantile. In this setting, the empirical mis-coverage frequency of previous examples serves as a signal of distribution shift or violation of exchangeability. \cite{gibbs2021adaptive} show that \textit{ACI} is capable of forming prediction sets that are robust to changes in the underlying data distribution. \cite{zaffran2022adaptive} extends \textit{ACI} to time-series data and introduce \textit{AgACI}, which is a parameter-free variant of \textit{ACI} that uses online expert aggregation to adaptively combine multiple ACI experts. In contrast to \textit{ACI}, where the learning rate should be carefully chosen in advance, \textit{AgACI} leverages a number of $k$ experts working with different learning rates, and automatically learns the optimal learning rate by aggregating across experts such that each expert's contribution is proportional to their corresponding performances over previous iterations. \cite{zaffran2022adaptive} show \textit{AgACI} demonstrates strong performance and produces tighter, well-calibrated prediction intervals consistently.  
\vspace{0.5em} 

Another line of work strives to find suitable and efficient conformity score functions tailored for specific tasks. For example, \cite{lei2018distribution} introduces a simple score for regression that accounts for heteroskedasticity, thus offering local adaptivity. \cite{romano2020classification} proposes a novel conformity score crafted for multi-label classification tasks. While providing marginal coverage, this score also demonstrates full adaptiveness to data complexities and enhances the approximated conditional coverage. In parallel, \cite{romano2019conformalized} develops a conformity score that involves both upper and lower bounds corresponding to a given coverage rate, enabling to combine the strengths of quantile regression and conformal prediction. Furthermore, \cite{xie2024boosted incorporates a boosting procedure between the training and calibration steps in order to enhance an arbitrary score function.

\vspace{0.5em} 
The underlying prediction algorithm also plays an important role in \gls{cp}, since the trained predictor serves as an approximation of the real data generation process and determines the base from which intervals are constructed. \gls{scp} fits the predictor only once, however potentially at a cost of training accuracy and  but statistical efficiency, since both training set and calibration set only see a subset of samples. One way to address this challenge is combining conformal prediction with techniques like cross-validation\cite{vovk2015cross}, Jackknife+ \cite{barber2020jackknife} and bootstrapping \cite{kim2020predictive}, such that all data points are used for both training and calibration, with only a limited number of model fits. 

\vspace{0.5em} 
In addition, there are works beyond the above three categories. Some works boost prediction reliability through enhancing the datasets. \cite{StutzTMLR2023} proposes a CP algorithm targeted on a special classification case where the ground-truth is ambiguous and consequently cause a biased distribution in manually-annotated labels. This method approximates the true distribution of labels by resampling data points using Monte-Carlo sampling. \cite{Shanmugam2025TTA} proposes to integrate a test-time data augmentation into \gls{cp} to reduce prediction set size and improve stability. Moreover, Conformal Risk Control \cite{angelopoulos2022conformal} also emerges as an important extension of \gls{cp} that not only provides distribution-free coverage guarantees but also explicitly controls risk. It is achieved by replacing the coverage indicator in Equation \ref{eq: finitecp} with a custom loss function. The prediction set is then bounding to the expected loss instead of the mis-coverage rate. Several works have adopted this risk-control approach such as \cite{mossina2024CVPR}.

% chapter 3
\include{methodology}

% chapter 4
\include{dataset}

% chapter 5
\chapter{Experiments and Results}
This chapter describes the performance of the methods presented in Chapter \ref{ch3}  applied to the datasets described in Chapter \ref{ch: dataset}. We first describe the general setups, which are shared across all experiments, then we present the configuration tuning strategy together with the implementation details. 

\section{Setups and Implementation}
As discussed in Chapter \ref{ch3}, we primarily run experiments for five different calibrated NAS algorithms: \gls{scp} with ensemble predictor, \gls{scp} with quantile regressor, \gls{cvcp} with ensemble predictor, \gls{cvcp} with quantile regressor, and \gls{btcp} with ensemble predictor. Consistent with \cite{white2019bananas}, each algorithms is assigned with a search budget of 150 epochs and uses the validation accuracy as the supervision signal to guide the search. We run each algorithm for 50 trials with different random seeds and then average the results.

Although the neural networks are pre-trained, there still remain numerous hyperparameters for each search algorithm, making it unrealistic to tune them all. Also, not all hyperparameters have the same impact on search performance; some might be more important than the others. Therefore, we select a subset of hyperparameters, which we believe or have tested that are less important, and fix their values throughout the experiments. Specifically, architectures are always path-encoded. As for the acquisition side, the number of candidates that the acquisition function evaluates in each iteration is fixed at 100 and the maximal mutation allowed for each model in case of a mutation strategy is 1. In addition, we follow \cite{white2019bananas} and output 10 architectures to mimic the parallelized evaluation procedure.

We tune the hyperparameters progressively. First, we focus on those configurations common to all algorithms, then we explore the hyperparameters specific to search algorithms in the respective sections. We start by conducting a thorough analysis on the baseline method, i.e., \gls{scp} with ensemble predictor, to find the optimal general setting, like the number of quantiles, the size of the initial dataset (before fitting the surrogate), the acquisition function and the sampling strategy, etc. Then, this optimal setting is applied to other more sophisticated approaches.

We borrow the implementation of \gls{bananas} from NASLib \footnote{https://github.com/automl/NASLib}, which is a modern Python-based framework for \gls{nas} that is developed by the AutoML Freiburg group. NASLib is modularized, enabling a relatively easy integration of the new calibration block. In NASLib, each \gls{nas} algorithm typically comprises a \textit{trainer} shared across algorithms for initiating search and evaluations and an \textit{optimizer} for encapsulating specific search logics, including an inherent \textit{predictor} if applicable. To be precise, we leverage their implementations for the \textit{trainer} and the ensemble neural network. Building on that, we implement new modules for the quantile regressor, the distribution estimators, and the calibration algorithms. In addition, we also modify the export functionalities in order to access more intermediate outputs, such as the estimated distribution at each iteration.

\section{Baseline}
We consider \gls{scp} with the ensemble predictor as the baseline calibrated strategy. 


% chapter 6
\chapter{Conclusion}
This chapter presents the central findings of this work as well as their critical discussions (Section \ref{sec: discussion}). Finally, it highlights limitations and corresponding opportunities for further research (Section \ref{sec: future_work}).

\section{Discussion}
\label{sec: discussion}

\section{Limitations and Future Work}
\label{sec: future_work}


% Reference
\bibliographystyle{plain}
\bibliography{references}

% Appendix
% List of algorithms/figures/tables
\listofalgorithms 
\listoffigures 
\listoftables
\printglossary[type=\acronymtype, title=Acronyms]

\appendix
\chapter{Program Code and Data Resources}
The source code and a documentation are available at the GitHub repository: \url{https://github.com/chengc823/Thesis.}
The datasets used for experiments and algorithm evaluations are sourced from the \href{https://github.com/automl/NASLib/tree/Develop}{NASLib repository}.

In case of access or permission issues to the private repository, please reach out at: chechen@mail.uni-mannheim.de.

\chapter{Additional Experimental Results}


% Declare non-Plagiarism and independent work
\backmatter
\chapter{Ehrenwörtliche Erklärung}
Ich versichere, dass ich die beiliegende Bachelor-, Master-, Seminar-, oder
Projektarbeit ohne Hilfe Dritter und ohne Benutzung anderer als der angegebenen
Quellen und in der untenstehenden Tabelle angegebenen Hilfsmittel angefertigt
und die den benutzten Quellen wörtlich oder inhaltlich entnommenen Stellen als
solche kenntlich gemacht habe. Diese Arbeit hat in gleicher oder ähnlicher Form
noch keiner Prüfungsbehörde vorgelegen. Ich bin mir bewusst, dass eine falsche
Erklärung rechtliche Folgen haben wird.

% Declare the use of AI tools.
\vspace{0.5cm}
  \textbf{Declaration of Used AI Tools} \\[.3em]
  \begin{tabularx}{\textwidth}{lXlc}
    \toprule
    Tool & Purpose & Where? & Useful? \\
    \midrule
    ChatGPT & Rephrasing & Throughout & + \\
    ChatGPT & Debugging LaTeX syntax errors  & Equation & + \\
    ChatGPT & Rendering LaTeX tables from Python frame & Tables & + \\
    \bottomrule
  \end{tabularx}
\end{center}

\vspace{2cm}
\noindent Unterschrift\\
\noindent Mannheim, den 30.07.2025 \hfill

\end{document}
