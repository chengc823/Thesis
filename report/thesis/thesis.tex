% Do not change document class, margins, fonts, etc.
\documentclass[a4paper,oneside,bibliography=totoc]{scrbook}
\setlength{\parindent}{25pt}
\newtheorem{definition}{Definition} \newtheorem{proposition}{Proposition}
\usepackage{appendix}

% Add packages
\usepackage[
colorlinks=true, urlcolor=blue, linkcolor=black, colorlinks,citecolor=green
]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{emptypage}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[justification=raggedright]{caption}
\captionsetup{labelformat=empty, textformat=empty}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{algorithm} % you can modify the algorithm style to your liking
\usepackage{algorithmic}
\usepackage{csquotes}
\renewcommand{\algorithmiccomment}[1]{\hfill\textit{// #1}}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[nopostdot, style=super, nogroupskip, nonumberlist, toc]{glossaries}
\makeglossaries

% Citation style
\usepackage{cite
\usepackage{csquotes}
\bibliographystyle{chicagoa}
\setcitestyle{authoryear,round,semicolon,aysep={},yysep={,}} \let\cite\citep

%---------------------------------------------------------------------------------------
\input{acronyms}

% Begin Documents
\begin{document}
\setlength{\skip\footins}{20pt}
% Cover page
\frontmatter \subject{Master Thesis} % change to appropriate type
\title{\LARGE 
	Uncertainty Calibration with Online Conformal Prediction in Neural Architecture Search: \\ 
	An Evaluation under the BANANAS Framework 
}
\author{
	Cheng Chen\\ (matriculation number 1662473)} \date{July 31, 2025
}
\publishers{
	{\small Submitted to}\\
	Data and Web Science Group\\Prof.\ Dr.\ Margret Keuper\\University of Mannheim\\
}
\maketitle

% Abstract
\chapter{Abstract}
Some contents


% Table of contents
\begingroup%
\hypersetup{hidelinks} % disable link color in TOC only
\tableofcontents%
\endgroup


% List of algorithms/figures/tables
\listofalgorithms 
\listoffigures 
\listoftables


% Body
\mainmatter  % start new numbering

% chapter 1
\chapter{Introduction}
\gls{bananascp} \\
\gls{bananas} \\
\gls{nas} \\
\gls{cp}
\gls{scp}
\gls{cvcp}
\gls{btcp} 

\label{ch:intro}

\section{Motivation}
\section{Related Work}
\section{Contributions and Limitations}
\section{Outline}
Having gained an overview of the research question and the background, the remainder of this thesis is organized as follows. First, Chapter 2 reviews the related works on neural architecture search, uncertainty quantification, and in particular, conformal prediction. In Chapter 3, after proposing a novel framework to incorporate uncertainty calibration into the architecture search process in Section \ref{sec:overview}, we describe its methodological steps in more detail. In Section \ref{sec:cp}, we identify different types of conformal prediction algorithms that are applicable for \gls{nas}, and consider the use of the underlying surrogate models. In Section \ref{sec:distest} and Section \ref{sec:acq}, we further examine how the calibrated predictions can be applied in a Bayesian optimization process. In Chapter 4, we describe the benchmark dataset used for conducting experiments and comparing algorithm performance against the state-of-the-art techniques. In Chapter 5, we present the experiment setups and provide interpretations of the results. Finally, Chapter 6 concludes this work and discusses potential future work.


% chapter 2
\chapter{Literature Review}
This chapter offers the technical background related to the research question of this work. We start by providing a comprehensive overview of \gls{nas} and introduce the three dimensions that characterize a \gls{nas} algorithm, followed by an anatomy of the high-performing search algorithm \gls{bananas}. Then, we review the existing uncertainty quantification techniques, with a focus on \gls{cp} algorithms, particularly those related to the novel framework we propose in Chapter~\ref{ch3}.

\section{Neural Architecture Search}
\subsection{Background}
In the recent decades, deep learning has achieved remarkable success in a variety of areas, including computer vision, natural language understanding, and machine translation. This success is partly attributed to the meticulously hand-crafted neural network architectures. With the rising demand for efficient architecture engineering in complex domains, \gls{nas} has emerged as a technique for automating the design of neural architectures tailored for specific tasks.

\gls{nas} has been a rapidly progressing research domain in the past years. Since the popular work based on reinforcement learning \cite{zoph2017neural}, many \gls{nas} algorithms leveraging different techniques have been proposed, including evolutionary algorithms \cite{maziarz2018evo-nas}, Bayesian optimization \cite{kandasamy2018neural}, and gradient descent \cite{liu2018darts}. In general, a \gls{nas} algorithm can be characterized by three key dimensions: search space, search strategy, and performance evaluation \cite{elsken2019neural, white2023survey, wistuba2019survey}. Figure \ref{fig: nas_overview} illustrates a typical architetcture search process.


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{figs/overview_nas.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Overview of Neural Architecture Search}
	\label{fig: nas_overview}
	\parbox{\linewidth}{
	 \vspace{0.5em}
 	 {\small \textit{Figure \ref{fig: nas_overview}:} Overview of an architecture search process. The search strategy iteratively selects architectures from a predefined search space $\mathcal{A}$. The performance estimation strategy evaluates the model performance on the target dataset and returns the performance to the search strategy.
	}
 	}
\end{figure}

\noindent Next, we provide definitions of the terms and review the research progress of each domain.
\begin{description}[leftmargin=0cm, listparindent=\parindent]
	\item [Search Space] A search space defines a set of architectures that the search algorithm is allowed to select. The search space is often the first step when setting up \gls{nas} and perhaps is also the most essential step, because
		the design of the search space represents an important trade-off between human bias and efficiency of search: a
		smaller search space incorporating more prior human knowledge and involving more manual decisions will enable
		\gls{nas} algorithms to find high-performing architectures more easily, in contrast a larger space with more primitive
		building blocks provides higher odds of discovering truly novel architectures \cite{white2023survey}. Common  search
		spaces range in size from a few thousand to over $10^{20}$.	
	
		There are four major categories of search spaces in the \gls{nas} literature \cite{white2023survey}. We start with two types of search spaces that have relatively simple architecture topologies. The macro search spaces \cite{baker2017designing, kandasamy2018neural, zoph2017neural} encode the entire neural architecture at a high level. Typically, an entire architecture is often represented by a \gls{dag}, with nodes defining the operation types and edges representing data flows. Each node is allowed to have distinct structures, such as convolution, pooling. As a result, macro search  spaces are highly flexible and possess high representation power. Another type is the chain-structured search spaces. As suggested by the name, chain-structured search spaces consist of neural networks that can be written as a sequence of operation layers. These search spaces often take state-of-the-art manual designs as the backbone. For example, there are several chain-structured search spaces based on the convolutional networks \cite{cai2018proxylessnas} or the transformer architectures \cite{xu2021nasbert}. 
		
		The third group is the cell-based search spaces, which perhaps are the most popular type of search spaces in \gls{nas} research. The cell-based search spaces are inspired by the fact that state-of-the-art human-designed architectures often consist of repeated blocks. For instance, the high-performing Transformer \cite{vaswani2017attention} contains 6 identical stacked encoder and decoder layers. Thus, instead of searching for the entire network architecture from scratch, \cite{zoph2018learning} propose to only search over relatively small cells, and stack the cells according to a predefined skeleton to form the overall architecture. Building on this idea, \cite{zoph2018learning} proposes the first modern cell-based search space, NASNet, which comprises of two types of cells: the normal cell that preserves the dimensionality and the reduction cell that reduces the spatial dimension. Since its emergence, many other cell-based search spaces have been developed. In general, these cell search spaces share a high-level similarity, but differ in the design of the fixed macro structure, the layout and constraints in the cells, and the choices of operations within the cells \cite{liu2018darts, dong2020nasbench201, pmlr-v97-ying19a}. The cell-based approach significantly reduces the size and the complexity of the search space. However, it has been criticized for limiting the expressiveness of \gls{nas}, potentially hindering the discovery of highly novel architectures \cite{white2023survey}.	
		
		The last main category is the hierarchical search spaces. Different from the aforementioned types of search spaces that mostly have a flat representation, hierarchical search spaces involve designing patterns at different levels, where each higher-level pattern is often represented as a \gls{dag} of lower-level patterns \cite{christoforidis2021novel, liu2018hierarchical}.
		
	In addition to the architecture topology, another important design accompanying a search space is the architecture encodings.
	
	  
	
	\item [Search Strategy] 
	\item [Performance Evaluation] The performance evaluation refers to the process of estimating the performance of architectures. The estimated performance is communicated back to the search algorithm to guide the next search. 

		The simplest performance estimation strategy is to fully train an architecture on the training data and then evaluate its performance on the validation data. However, training each architecture demands substantial computation resources and typically takes serval hours or days on a GPU. 
		
		\textcolor{red}{todo}


\end{description}}



\subsection{BANANAS}
\label{sec:bananas}

 includling.... BANANAs, a BO + porictor based strong performance..


\gls{bananas} is an Bayesian optimization based search strategy. 

We begin by an introduction of bayesian optimization...
Give an introduction how Bayesian optimization works. We list the five engineering decisions and review each field's related works. Maybe briefly cite Gaussian Process.

Bayesian optimization is a sequential decision-making process that seeks to find a global minimum x⋆ ∈ argminx∈X f(x) of an unknown black-box objective function f : X → R over an input space X ⊆ RD. 


\begin{description}[leftmargin=0cm, listparindent=\parindent]
	\item [Architecture Encoding]
	\item [Neural Predictor] \gls{fnns}
	\item [Uncertainty Estimation] 
	\item [Acquisition Function] The experiments explore five commonly used acquisition functions: \gls{ts}, \gls{its}, \gls{ucb}, \gls{pi}, and \gls{ei}. Each function is adapted to the Gaussian assumption, thereby requiring only the mean and standard deviation estimates to compute the acquisition scores. In the experiments, overall \gls{its} yields the best performance among all the options, although the marginal outperformance is subtle. The results indicate that the acquisition function does not have as significant impact on the search performance as the other examined components in the framework.
		
	\item [Acquisition Optimization] In each iteration of the Bayesian optimization, the goal is to select a candidate from the search space that maximizes the acquisition score. Evaluating the acquisition function for every architecture available in the search space is computationally infeasible, therefore \cite{white2019bananas} proposes to create a set of 100 to 1000 candidates and then choose the architecture with the maximal acquisition score in this set. Specifically, \cite{white2019bananas} explores various approaches for creating this candidate set. The simplest and most natural way is to draw architectures at random. Consider that architectures close in edit distance to those used for training the surrogate model are likely to have more accurate estimates, an alternative is a mutation-based sampling approach, where the candidate set is created via local search by randomly modifying an operation or an edge of the best-performing architectures that have been evaluated so far. In additional, \cite{white2019bananas} also examines a hybrid approach that combines random search with mutation-based search. Their experiments show that the mutation-based approach outperforms its competitors and suggest it is better to search locally rather than globally.

\end{description}}


\section{Uncertainty Quantification Methods}
Understanding uncertainty is important for real-world application of artificial intelligence, e.g., in autonomous driving, medical diagnosis. 

\begin{description}[leftmargin=0cm]
\item[Aleatory Uncertainty]  (data uncertainty): uncertainty that arises due to inherent variations and randomness, and cannot be reduced by collecting more information 

\item[Epistemic Uncertainty] (model uncertainty): uncertainty that arises due to lack of knowledge, and can be reduced by collecting more information.
\end{description}

- Bayesian-based: e.g., Bayesian Neural Network \\
- Ensemble-based: e.g., Monte-Carlo dropout \\
- Bootstrapping \\
But these techniques are limited in several perspectives. First, quantifying uncertainty requires training models for several times, which means that the models cannot be applied for real-time prediction or in an online-learning setup. Second, some models are pre-trained and are only accessible via API. Besides, models (pre-)trained on certain datasets may struggle to generalize across different domains or contexts. 

\section{Conformal Prediction}
\label{sec: reviewCP}

\subsection{Theoretical Background}
Starting from i.i.d data, and provide an intuitive demonstration how the prediction interval is constructed (can add a figure illustrating why conformal prediction works, i.e., symmetry). From the most intuitive expression to the finite-sample adjusted expression. 
		
\begin{description}[leftmargin=0cm]
	\item [Notation] Then, relax the i.i.d assumption to exchangeability, and lay a formal definition of the conformal prediction. And list the most importance three ingredients of the conformal predictions. \\
	- A trained predictor f \\
	- A conformity score function s. The conformity score is an important engineering decision and has an impact on the size on the prediction set, i.e., the efficiency. The conformity score function can be either a negatively- or positively oriented, in which … And it can be a random variable as well. \\
	-	A target coverage alpha	\\

		Marginal coverage is guaranteed regardless of the choices in dataset and black
		box model. Only the model predictions are required to apply the technique.
	\item [A Link to Statistical Testing] (clarify the relationship between conformal prediction and hypothesis testing) In this video (22:21), it is explained the intuition why conformal prediction guarantees the coverage, which is quite similar to the spirit of hypothesis testing. 
	
	     \begin{table}[h]
    		\centering
    		\caption{Comparison between CP and Hypothesis Testing}

    		\renewcommand{\arraystretch}{1.2}
   			\begin{tabular}{| m{6cm} | m{6cm} |}
     			\hline
      		   	\textbf{CP} & \textbf{Hypothesis Testing} \\
               	\hline
        		(desired) Coverage level & Confidence level \\
        	   	\hline
        		Nominal error level (1 - Coverage level) & Significance level \\
        		\hline
        		The conformity score of the new instance & p-value (is an empirical term) \\
        		\hline
    		\end{tabular}
        	\label{tab:comparison}
		\end{table}
	  	
		The coverage parameters which should be pre-set plays a similar role as the confidence interval in hypothesis testing. 
		Conformal prediction is like hypothesis testing with hypotheses: \\
			H0: test instance i conforms to the training instances. \\
			H1: test instance i does not conform to the training instances. \\
\end{description}}

\subsection{Full Conformal Prediction}
\gls{fcp}


\subsection{Extensions of Conformal Prediction}
Since the transductive version of \gls{cp} was first proposed in \cite{gammerman1998learning}, several variants have been developed with different computational complexities, formal guarantees, and practical applications.

To address the aforementioned inefficient computation problem of \gls{fcp}, Split Conformal Prediction (\gls{scp}), also known as Inductive Conformal Prediction (ICP), was first introduced in \cite{papadopoulos2002inductive} by replacing the transductive inference with inductive inference.  aims to learn a general prediction rule about the data using the observed records. Then, this rule can be applied directly to obtain predictions when new data arrives in sequence, without re-using the training data and retraining the model repeatedly. The main concept involves splitting the data into two non-overlapping subsets, designated for training and calibration, respectively. A predictive model is fit exclusively on the training set, then non-conformity measures are computed on the calibration set to determine the prediction interval's width. Due to its simplicity and computational efficiency, \gls{scp} is one of the most commonly used techniques in the \gls{cp} family. We delve into methodological steps of \gls{scp} with pseudo-code in Section \ref{sec:scp}.

--------

Limitations of split conformal predictions:
- Distribution shift. The conformal prediction is built on the core assumption of exchangeability, which means the data points are identically distributed. However, this assumption is hard to meet in real-world application. For example, with time-series data this assumption is generally violated due to the temporal relationships. 
- Adaptivity. Once the conformity scores are computed on the calibration set, the decision threshold is settled and is applied to all test datapoints, regardless of the intrinsic complexity of the exact example. It is desirable that the threshold can adapts to the difficulty of the problem and produce a larger prediction interval/set on hard-to-solve example and smaller prediction interval/set on easy-to-solve example. This limitation echoes with the characteristic of Conformal Prediction that the guaranteed coverage is only marginal over all datapoints but not conditional on a specific data points..

Variations of Conformal predictions have been proposed to overcome the limitations. There are three main streams:
- find an empirical coverage rate which leads to the desired coverage level. For example, if the desired coverage rate is 90%, the calibration set suggests that the alpha should be set to 93%, then 93% is used for generating prediction on the test set. 
- find an efficient conformity score: Alternatively, […] apply the conformal prediction in an online setting to dynamically incorporate the conformity score of new data points.
- find suitable predictor: The trained predictor can be just a poor approximation of the real data generation process.

Besides, […] proposes a CP algorithm that samples datapoints using Monte-Carlo sampling to approach the real distribution of labels in case the ground-truth is ambiguous and consequently cause a biased distribution in manually-annotated labels.



% chapter 3
\include{methodology}


% chapter 4
\chapter{Dataset}
\label{Dataset}
To compare the performance of BANANAS--CP with the original BANANAS algorithms and assess the role of uncertainty calibration, we run experiments on the widely used benchmark tabular dataset NAS-Bench-201 \cite{dong2020nasbench201}.

NAS-Bench-201 is a cell-based architecture search space. 





 Each cell has in total 4 nodes and 6 edges. The nodes in this search space correspond to the architecture’s feature maps and the edges represent the architectures operation, which are chosen from the operation set O = {1 × 1 conv. , 3 × 3 conv. , 3 × 3 avg. pooling , skip , zero} (see Figure 1). This search space contains in total 56 = 15 625 architectures

 search space defined in NAS-Bench-201 includes all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 neural cell candidates in total
 
Each of the architectures are trained on three datasets: CIFAR-10, CIFAR-100 and ImageNet-16-120. 

... descriptions of each dataset.



allows research focusing on the NAS algroithms and re, which also enables standardized comparison across NAS algorithms . Besides, NAS-Bench-201 also facilitates further extension, e.g., robustness dataset.


In this work, we leverge the trained models and simply query the validation accuracy of architectures from the search space.


% chapter 5
\chapter{Experiments and Results}
\section{Setup}
\section{Baseline}


% chapter 6
\chapter{Conclusion}
\label{Conclude}
This chapter presents the central findings of this work as well as their critical discussion. Finally, it highlights limitations and corresponding opportunities for further research.



% Reference
\bibliographystyle{plain}
\bibliography{references}

% Appendix
\appendix
\printglossary[type=\acronymtype, title=Acronyms]

\chapter{Program Code and Data Resources}
The source code and a documentation are available at the GitHub repository: \url{https://github.com/chengc823/Thesis.}
The datasets used for experiments and algorithm evaluations are sourced from the \href{https://github.com/automl/NASLib/tree/Develop}{NASLib repository.}

In case of access or permission issues to the private repository, please reach out at: chechen@mail.uni-mannheim.de.

\chapter{Additional Experimental Results}


% Declare non-Plagiarism and independent work
\backmatter
\chapter{Ehrenwörtliche Erklärung}
Ich versichere, dass ich die beiliegende Bachelor-, Master-, Seminar-, oder
Projektarbeit ohne Hilfe Dritter und ohne Benutzung anderer als der angegebenen
Quellen und in der untenstehenden Tabelle angegebenen Hilfsmittel angefertigt
und die den benutzten Quellen wörtlich oder inhaltlich entnommenen Stellen als
solche kenntlich gemacht habe. Diese Arbeit hat in gleicher oder ähnlicher Form
noch keiner Prüfungsbehörde vorgelegen. Ich bin mir bewusst, dass eine falsche
Erklärung rechtliche Folgen haben wird.

% Declare the use of AI tools.
\vspace{0.5cm}
  \textbf{Declaration of Used AI Tools} \\[.3em]
  \begin{tabularx}{\textwidth}{lXlc}
    \toprule
    Tool & Purpose & Where? & Useful? \\
    \midrule
    ChatGPT & Rephrasing & Throughout & + \\
    ChatGPT & Debugging LaTeX syntax errors  & Equation & + \\
    ChatGPT & Rendering LaTeX tables from Python frame & Tables & + \\
    \bottomrule
  \end{tabularx}
\end{center}

\vspace{2cm}
\noindent Unterschrift\\
\noindent Mannheim, den 31.07.2025 \hfill

\end{document}
