% Do not change document class, margins, fonts, etc.
\documentclass[a4paper,oneside,bibliography=totoc]{scrbook}
\setlength{\parindent}{25pt}
\newtheorem{definition}{Definition} \newtheorem{proposition}{Proposition}
\usepackage{appendix}

% Add packages
\usepackage[
colorlinks=true, urlcolor=blue, linkcolor=black, colorlinks,citecolor=green
]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{emptypage}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[justification=raggedright]{caption}
\captionsetup{labelformat=empty, textformat=empty}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{algorithm} % you can modify the algorithm style to your liking
\usepackage{algorithmic}
\usepackage{csquotes}
\renewcommand{\algorithmiccomment}[1]{\hfill\textit{// #1}}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[nopostdot, style=super, nogroupskip, nonumberlist, toc]{glossaries}
\makeglossaries

% Citation style
\usepackage{cite
\usepackage{csquotes}
\bibliographystyle{chicagoa}
\setcitestyle{authoryear,round,semicolon,aysep={},yysep={,}} \let\cite\citep

%---------------------------------------------------------------------------------------
\input{acronyms}

% Begin Documents
\begin{document}
\setlength{\skip\footins}{20pt}
% Cover page
\frontmatter \subject{Master Thesis} % change to appropriate type
\title{\LARGE 
	Uncertainty Calibration with Online Conformal Prediction in Neural Architecture Search: \\ 
	An Evaluation under the BANANAS Framework 
}
\author{
	Cheng Chen\\ (matriculation number 1662473)} \date{July 31, 2025
}
\publishers{
	{\small Submitted to}\\
	Data and Web Science Group\\Prof.\ Dr.\ Margret Keuper\\University of Mannheim\\
}
\maketitle

% Abstract
\chapter{Abstract}
Some contents


% Table of contents
\begingroup%
\hypersetup{hidelinks} % disable link color in TOC only
\tableofcontents%
\endgroup


% Body
\mainmatter  % start new numbering

% chapter 1
\chapter{Introduction}
\gls{bananascp} \\
\gls{bananas} \\
\gls{nas} \\
\gls{cp}
\gls{scp}
\gls{cvcp}
\gls{btcp} 

\label{ch:intro}

\section{Motivation}
\section{Related Work}
\section{Contributions and Limitations}
\section{Outline}
Having gained an overview of the research question and the background, the remainder of this thesis is organized as follows. First, Chapter 2 reviews the related works on neural architecture search, uncertainty quantification, and in particular, conformal prediction. In Chapter 3, after proposing a novel framework to incorporate uncertainty calibration into the architecture search process in Section \ref{sec:overview}, we describe its methodological steps in more detail. In Section \ref{sec:cp}, we identify different types of conformal prediction algorithms that are applicable for \gls{nas}, and consider the use of the underlying surrogate models. In Section \ref{sec:distest} and Section \ref{sec:acq}, we further examine how the calibrated predictions can be applied in a Bayesian optimization process. In Chapter 4, we describe the benchmark dataset for conducting experiments. In Chapter 5, we present the experiment setups and  compare the algorithm performances against the state-of-the-art techniques. We also  provide interpretations of the results. Finally, Chapter 6 and Chapter 7 conclude this work and discuss potential future directions.


% chapter 2
\chapter{Background}
This chapter offers the technical background related to the research question of this work. We start by providing a comprehensive overview of \gls{nas} and introduce the three dimensions that characterize a \gls{nas} algorithm, followed by an anatomy of the high-performing search algorithm \gls{bananas}. Then, we review the existing uncertainty quantification techniques, with a focus on \gls{cp} algorithms, particularly those related to the novel framework we propose in Chapter~\ref{ch3}.

\section{Neural Architecture Search}
\subsection{Overview}
\label{sec: nas}
In the recent decades, deep learning has achieved remarkable success in a variety of areas, including computer vision, natural language understanding, and machine translation. This success is partly attributed to the meticulously hand-crafted neural network architectures. With the rising demand for efficient architecture engineering in complex domains, \gls{nas} has emerged as a technique for automating the design of neural architectures for specific tasks.
 
\gls{nas} has been a rapidly progressing research domain in the past years. Since the seminal work that achieves competitive performance on CIFAR-10 \cite{zoph2017neural}, numerous \gls{nas} algorithms built on different techniques have been proposed. In general, \gls{nas} algorithms can be characterized by three key dimensions: search space, search strategy, and performance evaluation strategy \cite{elsken2019neural, white2023survey, wistuba2019survey}. Figure \ref{fig: nas_overview} illustrates a typical architetcture search process.


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{figs/overview_nas.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Overview of Neural Architecture Search}
	\label{fig: nas_overview}
	\parbox{\linewidth}{
	 \vspace{0.5em}
 	 {\small \textit{Figure \ref{fig: nas_overview}:} Overview of an architecture search process. The search strategy iteratively selects architectures from a predefined search space $\mathcal{A}$. The performance estimation strategy evaluates the model performance on the target dataset and returns the performance to the search strategy.
	}
 	}
\end{figure}

\noindent Next, we provide definitions of the terms and review the research progress of each domain.
\begin{description}[leftmargin=0cm, listparindent=\parindent]
	\item [Search Space] A search space defines a set of architectures that the search algorithm is allowed to select. The search space is often the first step when setting up \gls{nas} and perhaps is also the most essential step, because
		the design of the search space represents an important trade-off between human bias and efficiency of search: a
		smaller search space incorporating more prior human knowledge and involving more manual decisions will enable
		\gls{nas} algorithms to find high-performing architectures more easily, in contrast a larger space with more primitive
		building blocks provides higher odds of discovering truly novel architectures \cite{white2023survey}. Common  search
		spaces range in size from a few thousand to over $10^{20}$.	
	
		There are four major categories of search spaces in the \gls{nas} literature \cite{white2023survey}. We start with two types of search spaces that have relatively simple architecture topologies. The macro search spaces \cite{baker2017designing, kandasamy2018neural, zoph2017neural} encode the entire neural architecture at a high level. Typically, an entire architecture is often represented by a \gls{dag}, with nodes defining the operation types and edges representing data flows. Each node is allowed to have distinct structures, such as convolution, pooling. As a result, macro search  spaces are highly flexible and possess high representation power. Another type is the chain-structured search spaces. As suggested by the name, chain-structured search spaces consist of neural networks that can be written as a sequence of operation layers. These search spaces often take state-of-the-art manual designs as the backbone. For example, there are several chain-structured search spaces based on the convolutional networks \cite{cai2018proxylessnas} or the transformer architectures \cite{xu2021nasbert}. 
		
		The third group is the cell-based search spaces, which perhaps are the most popular type of search spaces in \gls{nas} research. The cell-based search spaces are inspired by the fact that state-of-the-art human-designed architectures often consist of repeated blocks. For instance, the high-performing Transformer \cite{vaswani2017attention} contains 6 identical stacked encoder and decoder layers. Thus, instead of searching for the entire network architecture from scratch, \cite{zoph2018learning} propose to only search over relatively small cells, and stack the cells according to a predefined skeleton to form the overall architecture. Building on this idea, \cite{zoph2018learning} proposes the first modern cell-based search space, NASNet, which comprises of two types of cells: the normal cell that preserves the dimensionality and the reduction cell that reduces the spatial dimension, as illustrated in Figure \ref{fig: cellss}. Since its emergence, many other cell-based search spaces have been developed. In general, these cell search spaces share a high-level similarity, but differ in the design of the fixed macro structure, the layout and constraints in the cells, and the choices of operations within the cells \cite{dong2020nasbench201, liu2018darts,  pmlr-v97-ying19a}. The cell-based approach significantly reduces the size and the complexity of the search space. However, it has been criticized for limiting the expressiveness of \gls{nas}, potentially hindering the discovery of highly novel architectures \cite{white2023survey}.	
		
		\begin{figure}[htbp]
			\centering
			\includegraphics[scale=0.4]{figs/overview_cell_based_ss.png}
			\refstepcounter{figure}
   			\addcontentsline{lof}{figure}{Figure~\thefigure: Overview of Cell-based Search Space}
			\label{fig: cellss}
			\parbox{\linewidth}{
	 		\vspace{0.5em}
 	 		{\small \textit{Figure \ref{fig: cellss}:} Overview of a cell-based search space NasNet. The outer skeleton across cells (left) is fixed, and the operations, represented by nodes, within the cells are searchable (right) \cite{white2023survey}.
			}
 		}
		\end{figure}
		
		The last main category is the hierarchical search spaces. Different from the aforementioned types of search spaces that mostly have a flat representation, hierarchical search spaces involve designing patterns at different levels, where each higher-level pattern is often represented as a \gls{dag} of lower-level patterns \cite{christoforidis2021novel, liu2018hierarchical}.
		
	In addition to the architecture topology, another important design accompanying a search space is the architecture encodings, because  many \gls{nas} algorithms require representations of the architectures to, for example, mutate an architecture or train a predictive model to extrapolate its performance. For search spaces that can be represented by a \gls{dag}, adjacency matrix is a commonly used encoding method. In addition, other encoding techniques, including graph-based encoding \cite{ning2020generic}, path-based encoding \cite{white2019bananas} and conditionally-structured encoding methods tailored for hierarchical search spaces have been proposed. \cite{white2020study} has shown that the effect of the encoding methods varies across different \gls{nas} subroutines.	
	  
	\item [Search Strategy] According to \cite{white2023survey}, there are generally two main categories of search strategies: black-box optimization based techniques and one-shot techniques. 
	 
		The black-box optimization based techniques largely overlap with another sub-area of AutoML: the hyperparameter tuning. Common techniques for hyperparameter tuning have been proven to be efficient for \gls{nas} as well, including reinforcement learning \cite{zoph2017neural, zoph2018learning}, evolutionary algorithms \cite{maziarz2018evo-nas,  real2019regularized}, gradient descent\cite{liu2018darts}, and etc. In particular, we take a close look at the search strategies based on Bayesian optimization, since they are closely related to the research question of this work. Specifically, initial Bayesian optimization based approaches typically use the Gaussian Process (GP) as the surrogate model \cite{kandasamy2018neural}. However, these algorithms often demonstrate under-performance compared to their competitors due to several limitations: 1) search spaces are usually high-dimensional, non-continuous, and graph-like; 2) GPs requires custom distance metrics among architectures, which involves a time-consuming matrix inversion step. Besides, GPs are difficult to scale since the computation complexity grows cubically with the number of observations. To address these challenges, a new framework that using a neural predictor as the surrogate model for Bayesian optimization has been proposed and demonstrated strong performance \cite{ma2019deep, springenberg2016bayesian, white2019bananas}. We review this framework in details in Section \ref{sec:bananas}.
		
		The one-shot techniques are introduced to avoid training each architecture from scratch, The key idea is to train a \textit{supernetwork} that comprises all possible architectures in the search space as subnetworks. Once a supernet is trained, each architecture from the search space can be evaluated by inheriting the weights from the corresponding subnet within the supernet \cite{pmlr-v80-bender18a, liu2018darts}.
		
	\item [Performance Evaluation] The performance evaluation refers to the process of estimating the performance of architectures. The estimated performance is communicated back to the search algorithm to guide the next search. The simplest performance estimation strategy is to fully train an architecture on the training data and then evaluate its performance on the validation data. However, training each architecture demands substantial computation resources and typically takes serval hours or days on a GPU. Consequently, many methods for speeding up the performance evaluation process for architectures have been proposed. One popular line of work is to predict the performance of neural networks before they are fully trained using the zero-cost proxies \cite{pmlr-v139-mellor21a}.
	
	In this work, we primarily run experiments on the benchmark dataset NAS-Bench-201 \cite{dong2020nasbench201}, which offers queryable validation and test accuracies for all architectures in the search space and thereby eliminates the need to train neural networks when simulating \gls{nas} experiments. Hence, we provide only a brief overview of this aspect and refer the readers to \cite{white2023survey} for a comprehensive introduction to the performance evaluation techniques.	
	\end{description}}



\subsection{Bayesian Optimization and BANANAS}
\label{sec:bananas}
As briefly mentioned in Section \ref{sec: nas}, the Bayesian optimization based \gls{nas} search strategy using a neural network as the surrogate model has shown strong performance. In particular, after identifying five components of this framework, \cite{white2019bananas} performs a thorough analysis on each component's effect towards the search performance and proposes a final algorithm, i.e., \gls{bananas}, based on both theoretical and empirical findings. This method is proven to be efficient, achieving state-of-the-art performance on popular NAS benchmarks 

In this section, we present a detailed review of the work by \cite{white2019bananas}. We start with the theoretical background and give an introduction into the Bayesian optimization method. Next, we walk through the five identified components and provide a summary of the experimental findings.

Bayesian optimization \cite{Mockus1978} is a sequential decision-making process that seeks to find the global maximum (minimum is the negation of the maximum) of an unknown black-box objective function $f : X \rightarrow R$ over an input space $X \subseteq R^D$. In a Bayesian optimization process, the unknown objective function $f$ is treated as a random function and the prior belief over $f$ is encoded by a surrogate model, usually a Gaussian Process or a Parzen-Tree Estimator \cite{bergstra2011algorithms}. At each iteration, the surrogate model updates the prior with the observations and forms a posterior probabilistic distribution of $f$. Then, the acquisition function, another key component that trades off exploration and exploitation in the process, evaluates a set of candidates based on the posterior distribution and picks the data point with the largest acquisition score for next query. Algorithm \ref{alg:BO} outlines this procedure.

\begin{algorithm}[t]
  \caption{Bayesian Optimization}
  \label{alg:BO}
  \begin{algorithmic}[1]
  \textbf{Input:} surrogate model $\mathcal{M}$, acquisition function $\phi$, objective function $\myfunc{f(\cdot)}$, number of iterations $T$. \vskip
  
  \STATE Initialize the set of observations: $D \leftarrow \emptyset$
  \FOR {$t$ in $1,...,T$}
  		\begin{enumerate}
    	    \itemsep0em 
			\item Fit surrogate model $\mathcal{M}$ to current observations set $\mathcal{D}_{t-1}$.
			\item Evaluate acquisition function and select the next point for query: \newline
				  $x_t = \operatorname*{argmax}_{x \in \mathcal{X}} \; \phi(x, \; \mathcal{M})$	
			\item Query the objective function: $y_t = f(x_t)$
			\item Update the observations set: \mathcal{D}_t \leftarrow \mathcal{D}_{t-1} \cup \{(x_t, y_t)\}
		\end{enumerate}
  \ENDFOR
  \STATE \textbf{Output:} $x^*=\operatorname*{argmax}_{t=1,...,T} f(x_{t})$    
 \end{algorithmic}
 \end{algorithm}

The acquisition function adopted in the original paper \cite{Mockus1978} is the \gls{ei}. Other popular alternatives include: \gls{ts}, \gls{its}, \gls{ucb}, and \gls{pi}. Different acquisition functions typically favor exploration and exploitation differently. Nevertheless, \cite{agnihotri2020exploring} shows that \gls{ei} is competitive in reaching the optimum value with comparably few iterations. 

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{figs/overview_bo.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Illustration of Bayesian Optimization}
	\label{fig: bo}
	\parbox{\linewidth}{
	\vspace{0.5em}
 	{\small \textit{Figure \ref{fig: bo}:} Example of Bayesian optimization with Gaussian Process as the surrogate and \gls{ei} as the acquisition function to explore the minimum of the objective function \cite{agnihotri2020exploring}.
 	}
 	}
\end{figure}

Next, we return to the NAS framework "Bayesian optimization + neural predictor". Now it becomes obvious that this framework is essentially an optimization task searching the maximum with a neural network as the surrogate and neural architectures in a search space being the inputs. Specifically, \cite{white2019bananas} identifies five critical components within the framework, which are listed as follows:

\begin{description}[leftmargin=0cm, listparindent=\parindent]
	\item [Architecture Encoding] This item refers to obtaining a vectorized representation of the architectures. Previous work often encode architectures using an adjacency matrix based approach, where nodes are assigned with an arbitrary ordering  and then binary features for all edges in the \gls{dag} are set to form the final representation.	Notably, the resulting representation of a specific architecture is not deterministic since the encoding  relies on an arbitrary indexing of the nodes. 
 
	In contrast, \cite{white2019bananas} proposes a novel path-based encoding mechanism with optional path truncation. This method simply checks if a path from the input node to the output node, expressed in terms of operations (e.g., input $\rightarrow$ 1$\times$1 conv $\rightarrow$ 3$\times$3 pool$\rightarrow$ output), is present in an architecture. The final encoding is a binary vector indicating which of the possible paths within a cell are present in the architecture. In this way, an architecture is always mapped to the same (though not necessarily unique) path encoding. Experiments show that the path-based encoding substantially increases the performance of neural predictors.
	
	\item [Neural Predictor] This item is about choosing an appropriate neural network for surrogate. A set of neural architectures and their corresponding validation accuracies are randomly sampled from the search space for training and comparing different neural predictors. Among all tested neural predictors, which include VAEs, GCNs, and FNNs with either the adjacency matrix or path-based encoding, FNNs with path encoding demonstrates the strongest performance.
	 
	\item [Uncertainty Estimation] Uncertainty estimates are required to form the probabilistic distribution. For Bayesian neural networks (BNNs), the posterior distribution is inferred over the nework weights. For an ensemble of \gls{fnns}, the distribution is inferred under the Gaussian assumption. The results show that an ensemble of even only 3 to 5 neural networks in general yields more reliable uncertainty estimates than BNNs.
	
	\item [Acquisition Function] In the experiments, five commonly used acquisition functions are examined: \gls{ts}, \gls{its}, \gls{ucb}, \gls{pi}, and \gls{ei}. Each function is adapted to the Gaussian assumption, thereby requiring only the mean and standard deviation estimates to compute the acquisition scores. In the experiments, overall \gls{its} yields the best performance among all the options, although the marginal outperformance is subtle. The results indicate that the acquisition function does not have as significant impact on the search performance as the other examined components in the framework.
		
	\item [Acquisition Optimization] In each iteration of the Bayesian optimization, the goal is to select a candidate from the search space that maximizes the acquisition score. Evaluating the acquisition function for every architecture available in the search space is computationally infeasible, therefore \cite{white2019bananas} proposes to create a set of 100 to 1000 candidates and then choose the architecture with the maximal acquisition score in this set. Specifically, \cite{white2019bananas} explores various approaches for creating this candidate set. The simplest and most natural way is to draw architectures at random. Consider that architectures close in edit distance to those used for training the surrogate model are likely to have more accurate estimates, an alternative is a mutation-based sampling approach, where the candidate set is created via local search by randomly modifying an operation or an edge of the best-performing architectures that have been evaluated so far. In additional, \cite{white2019bananas} also examines a hybrid approach that combines random search with mutation-based search. Their experiments show that the mutation-based approach outperforms its competitors and suggest it is better to search locally rather than globally.
\end{description}}

Finally, the best components found in the aforementioned analyses are transformed into the BANANAS algorithm, which composed of an ensemble of \gls{fnns} using the path encoding, and \gls{its} with a mutation strategy for acquisition (Figure \ref{fig: bananas}). Specifically, uncertainty is estimated based on the Gaussian assumption.


\begin{figure}[t]
	\centering
	\includegraphics[scale=0.48]{figs/overview_bananas.png}
	\refstepcounter{figure}
   	\addcontentsline{lof}{figure}{Figure~\thefigure: Diagram of the BANANAS framework}
	\label{fig: bananas}
	\vspace{0.5em} 
	\\
 	{\small \textit{Figure \ref{fig: bananas}:} Diagram of the BANANAS framework \cite{white2019bananas}.}}
\end{figure}

\section{Uncertainty Quantification Methods}
While \gls{bananas} employs a relatively simple heuristic for uncertainty estimation, there are many commonly used \gls{uq} techniques that are generally more theoretically grounded and sophisticated. In this section, we review some of these \gls{uq} methods and explain the rationale for choosing \gls{cp} for the development of our approach. Despite not directly tied to this work, this thesis aspires to offer a comprehensive overview of common \gls{uq} techniques applied in deep learning.

\gls{uq} is the science of quantitative characterization and estimation of uncertainties in both computational and real world applications. Formally, the source of uncertainty can be categorized into two types:

\begin{description}[leftmargin=0cm]
\item[Aleatory Uncertainty:] also known as data uncertainty, refers to uncertainty that arises due to inherent variations or randomness in a system and can not be reduced. \item[Epistemic Uncertainty:] also known as model uncertainty, refers to uncertainty that arises due to lack of knowledge, and can be reduced by better modeling or colloecting more data.
\end{description}

A family of methods for quantifying uncertainty directly model the full predictive distribution $p(y|x)$. One example is the Gaussian Process, which is typically parameterized by a kernel function and the predictive inference can be obtained in an analytical manner based on the observations. Another important example is the Bayesian Neural Network \cite{neal1996bayesian}, where each weight in the network is a random variable rather than a single number. In practice, the posterior distribution is intractable due to the integral operation and the high dimensionality, and can only be approximated using e.g.,  variational inference or Monte-Carlo sampling methods \cite{springenberg2016bayesian}, which requires multiple forward passes through the network. 

Like the inference stage in BNNs, some methods also rely on aggregating statistics from sub-networks, such as Monte-Carlo Dropout \cite{gal2016dropout} and Deep Ensemble \cite{lakshminarayanan2017simple}. The idea of Monte-Carlo Dropout is to apply \textit{dropout}, a regularization technique often used for preventing overfitting during the training time by randomly deactivating neurons, also at the inference time to get multiple predictions. Deep Ensemble involves using a network that outputs two values in the final layer, corresponding to the predicted mean $\mu(x)$ and variance $\sigma^2(x)$, respectively. This neural network is typically trained by minimizing a custom loss function in which $\mu(x)$ and $\sigma^2(x)$ have opposing effects. To avoid overfitting and lower estimation variance, multiple instances of the neural network are initialized with different weights and trained independently. Then, the final estimates are obtained by aggregating the predictions from all these instances. 

Another popular approach is Quantile Regression (QR) \cite{koenker1978regression}. Instead of modeling the full distribution, QR only models a sequence of discrete quantiles of the output. We present QR with more details in Section \ref{sec:cp}. 
\newline

However, these \gls{uq} techniques present several challenges in practice. First, 


quantifying uncertainty requires training models for several times, which means that the models cannot be applied for real-time prediction or in an online setting. Second, some models are pre-trained and are only accessible via API. Besides, models (pre-)trained on certain datasets may struggle to generalize across different domains or contexts. 
uncertainty calibration using another regressor. For example, BNN depends on the prior choice , and hard to converge.

Compared to the other approaches , advantages: distribution free, model agnostic, lightweighted....Conformal prediction .calibrates both epistemic and aleatoric uncertainties equally well. We present a formal introduction into the \gls{cp} framework in Section \ref{sec: reviewCP}. 

\section{Conformal Prediction}
\label{sec: reviewCP}

\subsection{Theoretical Background}
Starting from i.i.d data, and provide an intuitive demonstration how the prediction interval is constructed (can add a figure illustrating why conformal prediction works, i.e., symmetry). From the most intuitive expression to the finite-sample adjusted expression. 
		
\begin{description}[leftmargin=0cm]
	\item [Notation] Then, relax the i.i.d assumption to exchangeability, and lay a formal definition of the conformal prediction. And list the most importance three ingredients of the conformal predictions. \\
	- A trained predictor f \\
	- A conformity score function s. The conformity score is an important engineering decision and has an impact on the size on the prediction set, i.e., the efficiency. The conformity score function can be either a negatively- or positively oriented, in which … And it can be a random variable as well. \\
	-	A target coverage alpha	\\

		Marginal coverage is guaranteed regardless of the choices in dataset and black
		box model. Only the model predictions are required to apply the technique.
	\item [A Link to Statistical Testing] (clarify the relationship between conformal prediction and hypothesis testing) In this video (22:21), it is explained the intuition why conformal prediction guarantees the coverage, which is quite similar to the spirit of hypothesis testing. 
	
	     \begin{table}[h]
    		\centering
    		\caption{Comparison between CP and Hypothesis Testing}

    		\renewcommand{\arraystretch}{1.2}
   			\begin{tabular}{| m{6cm} | m{6cm} |}
     			\hline
      		   	\textbf{CP} & \textbf{Hypothesis Testing} \\
               	\hline
        		(desired) Coverage level & Confidence level \\
        	   	\hline
        		Nominal error level (1 - Coverage level) & Significance level \\
        		\hline
        		The conformity score of the new instance & p-value (is an empirical term) \\
        		\hline
    		\end{tabular}
        	\label{tab:comparison}
		\end{table}
	  	
		The coverage parameters which should be pre-set plays a similar role as the confidence interval in hypothesis testing. 
		Conformal prediction is like hypothesis testing with hypotheses: \\
			H0: test instance i conforms to the training instances. \\
			H1: test instance i does not conform to the training instances. \\
\end{description}}

\subsection{Full Conformal Prediction}
\gls{fcp}


\subsection{Extensions of Conformal Prediction}
Since the transductive version of \gls{cp} was first proposed in \cite{gammerman1998learning}, several variants have been developed with different computational complexities, formal guarantees, and practical applications.

To address the aforementioned inefficient computation problem of \gls{fcp}, Split Conformal Prediction (\gls{scp}), also known as Inductive Conformal Prediction (ICP), was first introduced in \cite{papadopoulos2002inductive} by replacing the transductive inference with inductive inference.  aims to learn a general prediction rule about the data using the observed records. Then, this rule can be applied directly to obtain predictions when new data arrives in sequence, without re-using the training data and retraining the model repeatedly. The main concept involves splitting the data into two non-overlapping subsets, designated for training and calibration, respectively. A predictive model is fit exclusively on the training set, then non-conformity measures are computed on the calibration set to determine the prediction interval's width. Due to its simplicity and computational efficiency, \gls{scp} is one of the most commonly used techniques in the \gls{cp} family. We delve into methodological steps of \gls{scp} with pseudo-code in Section \ref{sec:scp}.

--------

Limitations of split conformal predictions:
- Distribution shift. The conformal prediction is built on the core assumption of exchangeability, which means the data points are identically distributed. However, this assumption is hard to meet in real-world application. For example, with time-series data this assumption is generally violated due to the temporal relationships. 
- Adaptivity. Once the conformity scores are computed on the calibration set, the decision threshold is settled and is applied to all test datapoints, regardless of the intrinsic complexity of the exact example. It is desirable that the threshold can adapts to the difficulty of the problem and produce a larger prediction interval/set on hard-to-solve example and smaller prediction interval/set on easy-to-solve example. This limitation echoes with the characteristic of Conformal Prediction that the guaranteed coverage is only marginal over all datapoints but not conditional on a specific data points..

Variations of Conformal predictions have been proposed to overcome the limitations. There are three main streams:
- find an empirical coverage rate which leads to the desired coverage level. For example, if the desired coverage rate is 90%, the calibration set suggests that the alpha should be set to 93%, then 93% is used for generating prediction on the test set. 
- find an efficient conformity score: Alternatively, […] apply the conformal prediction in an online setting to dynamically incorporate the conformity score of new data points.
- find suitable predictor: The trained predictor can be just a poor approximation of the real data generation process.

Besides, […] proposes a CP algorithm that samples datapoints using Monte-Carlo sampling to approach the real distribution of labels in case the ground-truth is ambiguous and consequently cause a biased distribution in manually-annotated labels.



% chapter 3
\include{methodology}

% chapter 4
\include{dataset}

% chapter 5
\chapter{Experiments and Results}
This chapter describes the performance of the methods presented in Chapter \ref{ch3}  applied to the datasets described in Chapter \ref{ch: dataset}. We first describe the general setups, which are shared across all experiments, then we present the configuration tuning strategy together with the implementation details. 

\section{Setups and Implementation}
As discussed in Chapter \ref{ch3}, we primarily run experiments for five different calibrated NAS algorithms: \gls{scp} with ensemble predictor, \gls{scp} with quantile regressor, \gls{cvcp} with ensemble predictor, \gls{cvcp} with quantile regressor, and \gls{btcp} with ensemble predictor. Consistent with \cite{white2019bananas}, each algorithms is assigned with a search budget of 150 epochs and uses the validation accuracy as the supervision signal to guide the search. We run each algorithm for 50 trials with different random seeds and then average the results.

Although the neural networks are pre-trained, there still remain numerous hyperparameters for each search algorithm, making it unrealistic to tune them all. Also, not all hyperparameters have the same impact on search performance; some might be more important than the others. Therefore, we select a subset of hyperparameters, which we believe or have tested that are less important, and fix their values throughout the experiments. Specifically, architectures are always path-encoded. As for the acquisition side, the number of candidates that the acquisition function evaluates in each iteration is fixed at 100 and the maximal mutation allowed for each model in case of a mutation strategy is 1. In addition, we follow \cite{white2019bananas} and output 10 architectures to mimic the parallelized evaluation procedure.

We tune the hyperparameters progressively. First, we focus on those configurations common to all algorithms, then we explore the hyperparameters specific to search algorithms in the respective sections. We start by conducting a thorough analysis on the baseline method, i.e., \gls{scp} with ensemble predictor, to find the optimal general setting, like the number of quantiles, the size of the initial dataset (before fitting the surrogate), the acquisition function and the sampling strategy, etc. Then, this optimal setting is applied to other more sophisticated approaches.

We borrow the implementation of \gls{bananas} from NASLib \footnote{https://github.com/automl/NASLib}, which is a modularized repository and enables us to integrate new blocks relatively easily. To be precise, we leverage their implementations for the ensemble predictor and the modules for the \textit{trainer} and \textit{optimizer}. Building on that, we add new modules for the quantile regressor, the distribution estimators, and the calibration algorithms. In addition, we also modify the export functionalities so that we can access more intermediate outputs, such as the estimated distribution at each iteration.




\section{Baseline}
We consider \gls{scp} with the ensemble predictor as the baseline calibrated strategy. 


% chapter 6
\chapter{Conclusion}
\label{Conclude}
This chapter summarizes the central findings of this work and discusses the empirical implications.
% chapter 7
\chapter{Future Work}
At last, we review the limitations of this work and highlight the opportunities for further research in this chapter.


% Reference
\bibliographystyle{plain}
\bibliography{references}

% Appendix
% List of algorithms/figures/tables
\listofalgorithms 
\listoffigures 
\listoftables
\printglossary[type=\acronymtype, title=Acronyms]

\appendix
\chapter{Program Code and Data Resources}
The source code and a documentation are available at the GitHub repository: \url{https://github.com/chengc823/Thesis.}
The datasets used for experiments and algorithm evaluations are sourced from the \href{https://github.com/automl/NASLib/tree/Develop}{NASLib repository}.

In case of access or permission issues to the private repository, please reach out at: chechen@mail.uni-mannheim.de.

\chapter{Additional Experimental Results}


% Declare non-Plagiarism and independent work
\backmatter
\chapter{Ehrenwörtliche Erklärung}
Ich versichere, dass ich die beiliegende Bachelor-, Master-, Seminar-, oder
Projektarbeit ohne Hilfe Dritter und ohne Benutzung anderer als der angegebenen
Quellen und in der untenstehenden Tabelle angegebenen Hilfsmittel angefertigt
und die den benutzten Quellen wörtlich oder inhaltlich entnommenen Stellen als
solche kenntlich gemacht habe. Diese Arbeit hat in gleicher oder ähnlicher Form
noch keiner Prüfungsbehörde vorgelegen. Ich bin mir bewusst, dass eine falsche
Erklärung rechtliche Folgen haben wird.

% Declare the use of AI tools.
\vspace{0.5cm}
  \textbf{Declaration of Used AI Tools} \\[.3em]
  \begin{tabularx}{\textwidth}{lXlc}
    \toprule
    Tool & Purpose & Where? & Useful? \\
    \midrule
    ChatGPT & Rephrasing & Throughout & + \\
    ChatGPT & Debugging LaTeX syntax errors  & Equation & + \\
    ChatGPT & Rendering LaTeX tables from Python frame & Tables & + \\
    \bottomrule
  \end{tabularx}
\end{center}

\vspace{2cm}
\noindent Unterschrift\\
\noindent Mannheim, den 31.07.2025 \hfill

\end{document}
